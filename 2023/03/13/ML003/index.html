<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/site.jpg"><link rel="icon" href="/img/site.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content=""><meta name="author" content="John Doe"><meta name="keywords" content=""><meta name="description" content="线性回归的梯度下降 Goals在本实验中，您将:  使用梯度下降自动优化w和b的过程  Tools在本实验中，我们将使用:  NumPy，一个用于科学计算的流行库 Matplotlib，用于绘制数据的流行库 在本地目录的lab_utils.py文件中绘制例程  12345import math, copyimport numpy as npimport matplotlib.pyplot as p"><meta property="og:type" content="article"><meta property="og:title" content="线性回归的梯度下降"><meta property="og:url" content="https://xingyuanjie.top/2023/03/13/ML003/index.html"><meta property="og:site_name" content="Amicoyuan"><meta property="og:description" content="线性回归的梯度下降 Goals在本实验中，您将:  使用梯度下降自动优化w和b的过程  Tools在本实验中，我们将使用:  NumPy，一个用于科学计算的流行库 Matplotlib，用于绘制数据的流行库 在本地目录的lab_utils.py文件中绘制例程  12345import math, copyimport numpy as npimport matplotlib.pyplot as p"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313145346118.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313150848463.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313151251919.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313151329621.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313151450500.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313151857315.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313151947363.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313154012173.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313154210478.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313154333429.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313155614815.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313160112122.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313160446994.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313160944244.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313161516209.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313161636772.png"><meta property="og:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313161959222.png"><meta property="article:published_time" content="2023-03-13T06:37:12.000Z"><meta property="article:modified_time" content="2023-03-13T08:26:41.770Z"><meta property="article:author" content="John Doe"><meta property="article:tag" content="Tensorflow"><meta property="article:tag" content="Machine Learning"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://xingyuanjie.top/2023/03/13/ML003/image-20230313145346118.png"><title>线性回归的梯度下降 - Amicoyuan</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/androidstudio.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"xingyuanjie.top",root:"/",version:"1.8.12",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"g10sppACiB0iwBrOiERhucmg-MdYXbMMI",app_key:"f7eskymhpDIBDrODMFqlWwQU",server_url:null,path:"window.location.pathname"}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Amicoyuan</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/tools/"><i class="iconfont icon-playstation-fill"></i> 工具</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="线性回归的梯度下降"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-03-13 14:37" pubdate>2023年3月13日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 7.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 24 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">线性回归的梯度下降</h1><div class="markdown-body"><h2 id="线性回归的梯度下降"><a href="#线性回归的梯度下降" class="headerlink" title="线性回归的梯度下降"></a>线性回归的梯度下降</h2><p><img src="/2023/03/13/ML003/image-20230313145346118.png" srcset="/img/loading.gif" lazyload alt="image-20230313145346118"></p><h2 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h2><p>在本实验中，您将:</p><ul><li>使用梯度下降自动优化w和b的过程</li></ul><h2 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h2><p>在本实验中，我们将使用:</p><ul><li>NumPy，一个用于科学计算的流行库</li><li>Matplotlib，用于绘制数据的流行库</li><li>在本地目录的lab_utils.py文件中绘制例程</li></ul><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math, copy<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.style.use(<span class="hljs-string">&#x27;./deeplearning.mplstyle&#x27;</span>)<br><span class="hljs-keyword">from</span> lab_utils_uni <span class="hljs-keyword">import</span> plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients<br></code></pre></div></td></tr></table></figure><h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><p>让我们使用与之前相同的两个数据点——1000平方英尺的房子以30万美元的价格出售，2000平方英尺的房子以50万美元的价格出售。</p><table><thead><tr><th>Size(1000 sqft)</th><th>Price(1000s of dollars)</th></tr></thead><tbody><tr><td>1</td><td>300</td></tr><tr><td>2</td><td>500</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">#Load our data set</span><br>x_train = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>])	<span class="hljs-comment">#features</span><br>y_train = np.array([<span class="hljs-number">300.0</span>,<span class="hljs-number">500.0</span>])	<span class="hljs-comment">#target value</span><br></code></pre></div></td></tr></table></figure><h2 id="Compute-Cost"><a href="#Compute-Cost" class="headerlink" title="Compute_Cost"></a>Compute_Cost</h2><p>这是上一个实验室开发的。我们在这里还会用到它。</p><figure class="highlight plaintext"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs pyhton">#Function to calculate the cost<br>def compute_cost(x, y, w, b,):<br>	<br>	m = x.shape[0]<br>	cost = 0<br>	<br>	for i in range(m):<br>		f_wb = w * x[i] + b<br>		cost = cost + (f_wb - y[i])**2<br>	total_cost = 1 / (2 * m)*cost<br>	<br>	return total_cost<br></code></pre></div></td></tr></table></figure><h2 id="Gradient-descent-summary"><a href="#Gradient-descent-summary" class="headerlink" title="Gradient descent summary"></a>Gradient descent summary</h2><p>到目前为止，在这门课程中，你已经建立了一个线性模型来预测f_w,b(x^i):</p><p><img src="/2023/03/13/ML003/image-20230313150848463.png" srcset="/img/loading.gif" lazyload alt="image-20230313150848463"></p><p>在线性回归中，您使用输入训练数据来拟合参数𝑤,𝑏;来最小化我们的预测之间的误差测量f_𝑤，𝑏(𝑥^(𝑖))和实际数据𝑦(𝑖)。这种测量成为代价，J（w,b）。在训练中，你可以衡量我们所有训练样本的成本𝑥(𝑖)，𝑦(𝑖)。</p><p><img src="/2023/03/13/ML003/image-20230313151251919.png" srcset="/img/loading.gif" lazyload alt="image-20230313151251919"></p><p>在课堂上，梯度下降被描述为:</p><p><img src="/2023/03/13/ML003/image-20230313151329621.png" srcset="/img/loading.gif" lazyload alt="image-20230313151329621"></p><p>其中参数𝑤,𝑏同时更新。</p><p><img src="/2023/03/13/ML003/image-20230313151450500.png" srcset="/img/loading.gif" lazyload alt="image-20230313151450500"></p><p>这里同时意味着在更新任何参数之前计算所有参数的偏导数。</p><h2 id="Implement-Gradient-Descent"><a href="#Implement-Gradient-Descent" class="headerlink" title="Implement Gradient Descent"></a>Implement Gradient Descent</h2><p>你将为一个特征实现梯度下降算法。你需要三个函数。</p><ul><li>compute_gradient实现上述式(4)和(5)</li><li>上面的compute_cost实现方程(2)(代码来自以前的实验室)</li><li>gradient_descent，使用compute_gradient和compute_cost</li></ul><p>Conventions:</p><p><img src="/2023/03/13/ML003/image-20230313151857315.png" srcset="/img/loading.gif" lazyload alt="image-20230313151857315"></p><h2 id="compute-gradient"><a href="#compute-gradient" class="headerlink" title="compute_gradient"></a>compute_gradient</h2><p><img src="/2023/03/13/ML003/image-20230313151947363.png" srcset="/img/loading.gif" lazyload alt="image-20230313151947363"></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_gradient</span>(<span class="hljs-params">x, y, w, b</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Computes the gradient for linear regression</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">    	x (ndarray (m,)): Data, m examples</span><br><span class="hljs-string">    	y (ndarray (m,)): target values</span><br><span class="hljs-string">    	w,b (scalar)	: model parameters</span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    	dj_dw (scalar): The gradient of the cost w.r.t. the parameters w</span><br><span class="hljs-string">    	dj_db (scalar): The gradient of the cost w.r.t. the parameter b </span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    <span class="hljs-comment">#Number of training examples</span><br>    m = x.shape[<span class="hljs-number">0</span>]<br>    dj_de = <span class="hljs-number">0</span><br>    dj_db = <span class="hljs-number">0</span><br>    <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        f_wb = w * x[i] + b;<br>        dj_dw_i = (f_wb - y[i]) * x[i]<br>        dj_db_i = f_wb - y[i]<br>        dj_db += dj_db_i<br>        dj_dw += dj_dw_i<br>    dj_dw = dj_dw / m<br>    dj_db = dj_db / m<br>    <br>    <span class="hljs-keyword">return</span> dj_dw, dj_db<br></code></pre></div></td></tr></table></figure><p>课程描述了梯度下降如何利用在某一点上对参数代价的偏导数来更新该参数。</p><p>让我们使用compute_gradient函数来查找并绘制代价函数相对于其中一个参数𝑤0的偏导数。</p><p><img src="/2023/03/13/ML003/image-20230313154012173.png" srcset="/img/loading.gif" lazyload alt="image-20230313154012173"></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">plt_gradients(x_train,y_train, compute_cost, compute_gradient)<br>plt.show()<br></code></pre></div></td></tr></table></figure><p><img src="/2023/03/13/ML003/image-20230313154210478.png" srcset="/img/loading.gif" lazyload alt="image-20230313154210478"></p><p><img src="/2023/03/13/ML003/image-20230313154333429.png" srcset="/img/loading.gif" lazyload alt="image-20230313154333429"></p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>现在可以计算梯度，上面公式(3)中描述的梯度下降可以在下面的gradient_descent中实现。注释中描述了实现的细节。下面，您将利用这个函数在训练数据上找到w和b的最佳值。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gradient_descent</span>(<span class="hljs-params">x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function</span>):</span> <br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Performs gradient descent to fit w,b. Updates w,b by taking </span><br><span class="hljs-string">    num_iters gradient steps with learning rate alpha</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">      x (ndarray (m,))  : Data, m examples </span><br><span class="hljs-string">      y (ndarray (m,))  : target values</span><br><span class="hljs-string">      w_in,b_in (scalar): initial values of model parameters  </span><br><span class="hljs-string">      alpha (float):     Learning rate</span><br><span class="hljs-string">      num_iters (int):   number of iterations to run gradient descent</span><br><span class="hljs-string">      cost_function:     function to call to produce cost</span><br><span class="hljs-string">      gradient_function: function to call to produce gradient</span><br><span class="hljs-string">      </span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">      w (scalar): Updated value of parameter after running gradient descent</span><br><span class="hljs-string">      b (scalar): Updated value of parameter after running gradient descent</span><br><span class="hljs-string">      J_history (List): History of cost values</span><br><span class="hljs-string">      p_history (list): History of parameters [w,b] </span><br><span class="hljs-string">      &quot;&quot;&quot;</span><br>    <br>    w = copy.deepcopy(w_in) <span class="hljs-comment"># avoid modifying global w_in</span><br>    <span class="hljs-comment"># An array to store cost J and w&#x27;s at each iteration primarily for graphing later</span><br>    J_history = []<br>    p_history = []<br>    b = b_in<br>    w = w_in<br>    <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iters):<br>        <span class="hljs-comment"># Calculate the gradient and update the parameters using gradient_function</span><br>        dj_dw, dj_db = gradient_function(x, y, w , b)     <br><br>        <span class="hljs-comment"># Update Parameters using equation (3) above</span><br>        b = b - alpha * dj_db                            <br>        w = w - alpha * dj_dw                            <br><br>        <span class="hljs-comment"># Save cost J at each iteration</span><br>        <span class="hljs-keyword">if</span> i&lt;<span class="hljs-number">100000</span>:      <span class="hljs-comment"># prevent resource exhaustion </span><br>            J_history.append( cost_function(x, y, w , b))<br>            p_history.append([w,b])<br>        <span class="hljs-comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span><br>        <span class="hljs-keyword">if</span> i% math.ceil(num_iters/<span class="hljs-number">10</span>) == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteration <span class="hljs-subst">&#123;i:<span class="hljs-number">4</span>&#125;</span>: Cost <span class="hljs-subst">&#123;J_history[-<span class="hljs-number">1</span>]:<span class="hljs-number">0.2</span>e&#125;</span> &quot;</span>,<br>                  <span class="hljs-string">f&quot;dj_dw: <span class="hljs-subst">&#123;dj_dw: <span class="hljs-number">0.3</span>e&#125;</span>, dj_db: <span class="hljs-subst">&#123;dj_db: <span class="hljs-number">0.3</span>e&#125;</span>  &quot;</span>,<br>                  <span class="hljs-string">f&quot;w: <span class="hljs-subst">&#123;w: <span class="hljs-number">0.3</span>e&#125;</span>, b:<span class="hljs-subst">&#123;b: <span class="hljs-number">0.5</span>e&#125;</span>&quot;</span>)<br> <br>    <span class="hljs-keyword">return</span> w, b, J_history, p_history <span class="hljs-comment">#return w and J,w history for graphing</span><br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">Iteration    <span class="hljs-number">0</span>: Cost <span class="hljs-number">7.93e+04</span>  dj_dw: -<span class="hljs-number">6.500e+02</span>, dj_db: -<span class="hljs-number">4.000e+02</span>   w:  <span class="hljs-number">6.500e+00</span>, b: <span class="hljs-number">4.00000e+00</span><br>Iteration <span class="hljs-number">1000</span>: Cost <span class="hljs-number">3.41e+00</span>  dj_dw: -<span class="hljs-number">3.712e-01</span>, dj_db:  <span class="hljs-number">6.007e-01</span>   w:  <span class="hljs-number">1.949e+02</span>, b: <span class="hljs-number">1.08228e+02</span><br>Iteration <span class="hljs-number">2000</span>: Cost <span class="hljs-number">7.93e-01</span>  dj_dw: -<span class="hljs-number">1.789e-01</span>, dj_db:  <span class="hljs-number">2.895e-01</span>   w:  <span class="hljs-number">1.975e+02</span>, b: <span class="hljs-number">1.03966e+02</span><br>Iteration <span class="hljs-number">3000</span>: Cost <span class="hljs-number">1.84e-01</span>  dj_dw: -<span class="hljs-number">8.625e-02</span>, dj_db:  <span class="hljs-number">1.396e-01</span>   w:  <span class="hljs-number">1.988e+02</span>, b: <span class="hljs-number">1.01912e+02</span><br>Iteration <span class="hljs-number">4000</span>: Cost <span class="hljs-number">4.28e-02</span>  dj_dw: -<span class="hljs-number">4.158e-02</span>, dj_db:  <span class="hljs-number">6.727e-02</span>   w:  <span class="hljs-number">1.994e+02</span>, b: <span class="hljs-number">1.00922e+02</span><br>Iteration <span class="hljs-number">5000</span>: Cost <span class="hljs-number">9.95e-03</span>  dj_dw: -<span class="hljs-number">2.004e-02</span>, dj_db:  <span class="hljs-number">3.243e-02</span>   w:  <span class="hljs-number">1.997e+02</span>, b: <span class="hljs-number">1.00444e+02</span><br>Iteration <span class="hljs-number">6000</span>: Cost <span class="hljs-number">2.31e-03</span>  dj_dw: -<span class="hljs-number">9.660e-03</span>, dj_db:  <span class="hljs-number">1.563e-02</span>   w:  <span class="hljs-number">1.999e+02</span>, b: <span class="hljs-number">1.00214e+02</span><br>Iteration <span class="hljs-number">7000</span>: Cost <span class="hljs-number">5.37e-04</span>  dj_dw: -<span class="hljs-number">4.657e-03</span>, dj_db:  <span class="hljs-number">7.535e-03</span>   w:  <span class="hljs-number">1.999e+02</span>, b: <span class="hljs-number">1.00103e+02</span><br>Iteration <span class="hljs-number">8000</span>: Cost <span class="hljs-number">1.25e-04</span>  dj_dw: -<span class="hljs-number">2.245e-03</span>, dj_db:  <span class="hljs-number">3.632e-03</span>   w:  <span class="hljs-number">2.000e+02</span>, b: <span class="hljs-number">1.00050e+02</span><br>Iteration <span class="hljs-number">9000</span>: Cost <span class="hljs-number">2.90e-05</span>  dj_dw: -<span class="hljs-number">1.082e-03</span>, dj_db:  <span class="hljs-number">1.751e-03</span>   w:  <span class="hljs-number">2.000e+02</span>, b: <span class="hljs-number">1.00024e+02</span><br>(w,b) found by gradient descent: (<span class="hljs-number">199.9929</span>,<span class="hljs-number">100.0116</span>)<br></code></pre></div></td></tr></table></figure><p>花点时间，注意上面打印的梯度下降过程的一些特征。</p><ul><li>正如课堂上的幻灯片所描述的，成本开始很大，然后迅速下降。</li><li>偏导数dj_dw和dj_db也变小了，起初很快，然后变慢。正如课堂上的图表所示，随着过程接近“碗底”，由于在这一点上的导数值较小，进程会变慢。</li><li>尽管学习率alpha保持不变，但进程会减慢</li></ul><p><img src="/2023/03/13/ML003/image-20230313155614815.png" srcset="/img/loading.gif" lazyload alt="image-20230313155614815"></p><h2 id="Cost-versus-iterations-of-gradient-descent"><a href="#Cost-versus-iterations-of-gradient-descent" class="headerlink" title="Cost versus iterations of gradient descent"></a>Cost versus iterations of gradient descent</h2><p>成本与迭代的关系图是衡量梯度下降技术进展的有用方法。在成功的运行中，成本总是会降低。最初成本的变化如此之快，用不同于最终下降的尺度来描绘最初的上升是有用的。</p><p>在下面的图表中，请注意轴上的成本规模和迭代步骤。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># plot cost versus iteration  </span><br>fig, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, constrained_layout=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">4</span>))<br>ax1.plot(J_hist[:<span class="hljs-number">100</span>])<br>ax2.plot(<span class="hljs-number">1000</span> + np.arange(<span class="hljs-built_in">len</span>(J_hist[<span class="hljs-number">1000</span>:])), J_hist[<span class="hljs-number">1000</span>:])<br>ax1.set_title(<span class="hljs-string">&quot;Cost vs. iteration(start)&quot;</span>);  ax2.set_title(<span class="hljs-string">&quot;Cost vs. iteration (end)&quot;</span>)<br>ax1.set_ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)            ;  ax2.set_ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>) <br>ax1.set_xlabel(<span class="hljs-string">&#x27;iteration step&#x27;</span>)  ;  ax2.set_xlabel(<span class="hljs-string">&#x27;iteration step&#x27;</span>) <br>plt.show()<br></code></pre></div></td></tr></table></figure><p><img src="/2023/03/13/ML003/image-20230313160112122.png" srcset="/img/loading.gif" lazyload alt="image-20230313160112122"></p><h2 id="Predictions"><a href="#Predictions" class="headerlink" title="Predictions"></a>Predictions</h2><p>现在您已经发现了参数𝑤的最佳值和𝑏，您现在可以使用该模型根据我们学习的参数来预测房屋价值。正如预期的那样，在相同的住房条件下，预测值与训练值几乎相同。进一步，不在预测中的值与期望值一致。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;1000 sqft house prediction <span class="hljs-subst">&#123;w_final*<span class="hljs-number">1.0</span> + b_final:<span class="hljs-number">0.1</span>f&#125;</span> Thousand dollars&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;1200 sqft house prediction <span class="hljs-subst">&#123;w_final*<span class="hljs-number">1.2</span> + b_final:<span class="hljs-number">0.1</span>f&#125;</span> Thousand dollars&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;2000 sqft house prediction <span class="hljs-subst">&#123;w_final*<span class="hljs-number">2.0</span> + b_final:<span class="hljs-number">0.1</span>f&#125;</span> Thousand dollars&quot;</span>)<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-number">1000</span> sqft house prediction <span class="hljs-number">300.0</span> Thousand dollars<br><span class="hljs-number">1200</span> sqft house prediction <span class="hljs-number">340.0</span> Thousand dollars<br><span class="hljs-number">2000</span> sqft house prediction <span class="hljs-number">500.0</span> Thousand dollars<br></code></pre></div></td></tr></table></figure><h2 id="Plotting"><a href="#Plotting" class="headerlink" title="Plotting"></a>Plotting</h2><p>您可以通过在代价(w,b)的等高线图上绘制迭代的代价来显示梯度下降在执行过程中的进度。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">fig, ax = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))<br>plt_contour_wgrad(x_train, y_train, p_hist, ax)<br></code></pre></div></td></tr></table></figure><p><img src="/2023/03/13/ML003/image-20230313160446994.png" srcset="/img/loading.gif" lazyload alt="image-20230313160446994"></p><p>上面的等高线图显示了𝑤和𝑏范围内的𝑐𝑜𝑠𝑡(𝑤，𝑏)。成本水平由圆环表示。用红色箭头覆盖的是梯度下降的路径。这里有一些需要注意的事情:</p><ul><li>这条路径朝着它的目标稳步(单调)前进。</li><li>最初的步骤比接近目标的步骤要大得多。</li></ul><p>放大，我们可以看到梯度下降的最后步骤。注意，阶梯之间的距离随着梯度趋近于零而缩小。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">fig, ax = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">4</span>))<br>plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[<span class="hljs-number">180</span>, <span class="hljs-number">220</span>, <span class="hljs-number">0.5</span>], b_range=[<span class="hljs-number">80</span>, <span class="hljs-number">120</span>, <span class="hljs-number">0.5</span>],<br>            contours=[<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">10</span>,<span class="hljs-number">20</span>],resolution=<span class="hljs-number">0.5</span>)<br></code></pre></div></td></tr></table></figure><p><img src="/2023/03/13/ML003/image-20230313160944244.png" srcset="/img/loading.gif" lazyload alt="image-20230313160944244"></p><h2 id="Increased-Learning-Rate"><a href="#Increased-Learning-Rate" class="headerlink" title="Increased Learning Rate"></a>Increased Learning Rate</h2><p>在这节课中，在式(3)中有一个关于学习率的合适值𝛼的讨论。𝛼越大，梯度下降收敛到解的速度就越快。但是，如果它太大，梯度下降就会发散。上面有一个很好收敛的解的例子。让我们试着增加𝛼的值看看会发生什么:</p><p><img src="/2023/03/13/ML003/image-20230313161516209.png" srcset="/img/loading.gif" lazyload alt="image-20230313161516209"></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># initialize parameters</span><br>w_init = <span class="hljs-number">0</span><br>b_init = <span class="hljs-number">0</span><br><span class="hljs-comment"># set alpha to a large value</span><br>iterations = <span class="hljs-number">10</span><br>tmp_alpha = <span class="hljs-number">8.0e-1</span><br><span class="hljs-comment"># run gradient descent</span><br>w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, <br>                                                    iterations, compute_cost, compute_gradient)<br></code></pre></div></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs pyhton">Iteration    0: Cost 2.58e+05  dj_dw: -6.500e+02, dj_db: -4.000e+02   w:  5.200e+02, b: 3.20000e+02<br>Iteration    1: Cost 7.82e+05  dj_dw:  1.130e+03, dj_db:  7.000e+02   w: -3.840e+02, b:-2.40000e+02<br>Iteration    2: Cost 2.37e+06  dj_dw: -1.970e+03, dj_db: -1.216e+03   w:  1.192e+03, b: 7.32800e+02<br>Iteration    3: Cost 7.19e+06  dj_dw:  3.429e+03, dj_db:  2.121e+03   w: -1.551e+03, b:-9.63840e+02<br>Iteration    4: Cost 2.18e+07  dj_dw: -5.974e+03, dj_db: -3.691e+03   w:  3.228e+03, b: 1.98886e+03<br>Iteration    5: Cost 6.62e+07  dj_dw:  1.040e+04, dj_db:  6.431e+03   w: -5.095e+03, b:-3.15579e+03<br>Iteration    6: Cost 2.01e+08  dj_dw: -1.812e+04, dj_db: -1.120e+04   w:  9.402e+03, b: 5.80237e+03<br>Iteration    7: Cost 6.09e+08  dj_dw:  3.156e+04, dj_db:  1.950e+04   w: -1.584e+04, b:-9.80139e+03<br>Iteration    8: Cost 1.85e+09  dj_dw: -5.496e+04, dj_db: -3.397e+04   w:  2.813e+04, b: 1.73730e+04<br>Iteration    9: Cost 5.60e+09  dj_dw:  9.572e+04, dj_db:  5.916e+04   w: -4.845e+04, b:-2.99567e+04<br></code></pre></div></td></tr></table></figure><p><img src="/2023/03/13/ML003/image-20230313161636772.png" srcset="/img/loading.gif" lazyload alt="image-20230313161636772"></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">plt_divergence(p_hist, J_hist,x_train, y_train)<br>plt.show()<br></code></pre></div></td></tr></table></figure><p><img src="/2023/03/13/ML003/image-20230313161959222.png" srcset="/img/loading.gif" lazyload alt="image-20230313161959222"></p><p>上图中，左图显示了𝑤在梯度下降的前几个步骤中的进展。𝑤从正振荡到负，成本迅速增长。梯度下降同时在𝑤和𝑏上运行，所以需要右边的3d图才能看到完整的图片。</p><h2 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h2><p>在这个实验室里，你:</p><ul><li>深入研究单个变量的梯度下降的细节。</li><li>开发了一个计算梯度的程序</li><li>看看梯度是什么</li><li>完成一个梯度下降程序</li><li>利用梯度下降法寻找参数</li><li>检查了学习率大小的影响</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Pa411X76s?p=5&amp;vd_source=3ae32e36058f58c5b85935fca9b77797">https://www.bilibili.com/video/BV1Pa411X76s?p=5&amp;vd_source=3ae32e36058f58c5b85935fca9b77797</a></p><p><a target="_blank" rel="noopener" href="https://github.com/kaieye/2022-Machine-Learning-Specialization">kaieye&#x2F;2022-Machine-Learning-Specialization (github.com)</a></p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/Machine-Learning/">Machine Learning</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/Tensorflow/">Tensorflow</a> <a class="hover-with-bg" href="/tags/Machine-Learning/">Machine Learning</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p><div class="post-prevnext"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/2023/03/12/ML002/"><span class="hidden-mobile">代价函数</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="waline"></div><script type="text/javascript">Fluid.utils.loadComments("#waline",(function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/@waline/client@1/dist/Waline.min.js",(function(){var i=Object.assign({serverURL:"https://example.xingyuanjie.top/",path:"window.location.pathname",placeholder:"欢迎留言~(填写邮箱可在被回复时收到邮件提醒哦)",meta:["nick","mail","link"],requiredMeta:["nick"],lang:"zh-CN",emoji:["https://cdn.jsdelivr.net/gh/walinejs/emojis/weibo"],dark:'html[data-user-color-scheme="dark"]',avatar:"retro",avatarCDN:"https://seccdn.libravatar.org/avatar/",avatarForce:!1,wordLimit:0,pageSize:10,highlight:!0},{el:"#waline",path:window.location.pathname});new Waline(i),Fluid.utils.waitElementVisible("#waline .vcontent",()=>{Fluid.plugins.initFancyBox("#waline .vcontent img:not(.vemoji)")})}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/local-search.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js"></script><script>!function(t,i){(0,Fluid.plugins.typing)(i.getElementById("subtitle").title)}(window,document)</script><script src="/js/boot.js"></script></body></html>