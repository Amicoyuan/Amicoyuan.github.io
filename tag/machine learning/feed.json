{
    "version": "https://jsonfeed.org/version/1",
    "title": "Amicoyuan • All posts by \"machine learning\" tag",
    "description": "",
    "home_page_url": "https://xingyuanjie.top",
    "items": [
        {
            "id": "https://xingyuanjie.top/2023/03/13/ML003/",
            "url": "https://xingyuanjie.top/2023/03/13/ML003/",
            "title": "线性回归的梯度下降",
            "date_published": "2023-03-13T06:37:12.000Z",
            "content_html": "<h2 id=\"线性回归的梯度下降\"><a href=\"#线性回归的梯度下降\" class=\"headerlink\" title=\"线性回归的梯度下降\"></a>线性回归的梯度下降</h2><p><img src=\"/2023/03/13/ML003/image-20230313145346118.png\" alt=\"image-20230313145346118\"></p>\n<h2 id=\"Goals\"><a href=\"#Goals\" class=\"headerlink\" title=\"Goals\"></a>Goals</h2><p>在本实验中，您将:</p>\n<ul>\n<li>使用梯度下降自动优化w和b的过程</li>\n</ul>\n<h2 id=\"Tools\"><a href=\"#Tools\" class=\"headerlink\" title=\"Tools\"></a>Tools</h2><p>在本实验中，我们将使用:</p>\n<ul>\n<li>NumPy，一个用于科学计算的流行库</li>\n<li>Matplotlib，用于绘制数据的流行库</li>\n<li>在本地目录的lab_utils.py文件中绘制例程</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">import</span> math, copy<br><span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np<br><span class=\"hljs-keyword\">import</span> matplotlib.pyplot <span class=\"hljs-keyword\">as</span> plt<br>plt.style.use(<span class=\"hljs-string\">&#x27;./deeplearning.mplstyle&#x27;</span>)<br><span class=\"hljs-keyword\">from</span> lab_utils_uni <span class=\"hljs-keyword\">import</span> plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients<br></code></pre></div></td></tr></table></figure>\n\n<h2 id=\"Problem-Statement\"><a href=\"#Problem-Statement\" class=\"headerlink\" title=\"Problem Statement\"></a>Problem Statement</h2><p>让我们使用与之前相同的两个数据点——1000平方英尺的房子以30万美元的价格出售，2000平方英尺的房子以50万美元的价格出售。</p>\n<table>\n<thead>\n<tr>\n<th>Size(1000 sqft)</th>\n<th>Price(1000s of dollars)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>300</td>\n</tr>\n<tr>\n<td>2</td>\n<td>500</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\">#Load our data set</span><br>x_train = np.array([<span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">2.0</span>])\t<span class=\"hljs-comment\">#features</span><br>y_train = np.array([<span class=\"hljs-number\">300.0</span>,<span class=\"hljs-number\">500.0</span>])\t<span class=\"hljs-comment\">#target value</span><br></code></pre></div></td></tr></table></figure>\n\n<h2 id=\"Compute-Cost\"><a href=\"#Compute-Cost\" class=\"headerlink\" title=\"Compute_Cost\"></a>Compute_Cost</h2><p>这是上一个实验室开发的。我们在这里还会用到它。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs pyhton\">#Function to calculate the cost<br>def compute_cost(x, y, w, b,):<br>\t<br>\tm = x.shape[0]<br>\tcost = 0<br>\t<br>\tfor i in range(m):<br>\t\tf_wb = w * x[i] + b<br>\t\tcost = cost + (f_wb - y[i])**2<br>\ttotal_cost = 1 / (2 * m)*cost<br>\t<br>\treturn total_cost<br></code></pre></div></td></tr></table></figure>\n\n<h2 id=\"Gradient-descent-summary\"><a href=\"#Gradient-descent-summary\" class=\"headerlink\" title=\"Gradient descent summary\"></a>Gradient descent summary</h2><p>到目前为止，在这门课程中，你已经建立了一个线性模型来预测f_w,b(x^i):</p>\n<p><img src=\"/2023/03/13/ML003/image-20230313150848463.png\" alt=\"image-20230313150848463\"></p>\n<p>在线性回归中，您使用输入训练数据来拟合参数𝑤,𝑏;来最小化我们的预测之间的误差测量f_𝑤，𝑏(𝑥^(𝑖))和实际数据𝑦(𝑖)。这种测量成为代价，J（w,b）。在训练中，你可以衡量我们所有训练样本的成本𝑥(𝑖)，𝑦(𝑖)。</p>\n<p><img src=\"/2023/03/13/ML003/image-20230313151251919.png\" alt=\"image-20230313151251919\"></p>\n<p>在课堂上，梯度下降被描述为:</p>\n<p><img src=\"/2023/03/13/ML003/image-20230313151329621.png\" alt=\"image-20230313151329621\"></p>\n<p>其中参数𝑤,𝑏同时更新。</p>\n<p><img src=\"/2023/03/13/ML003/image-20230313151450500.png\" alt=\"image-20230313151450500\"></p>\n<p>这里同时意味着在更新任何参数之前计算所有参数的偏导数。</p>\n<h2 id=\"Implement-Gradient-Descent\"><a href=\"#Implement-Gradient-Descent\" class=\"headerlink\" title=\"Implement Gradient Descent\"></a>Implement Gradient Descent</h2><p>你将为一个特征实现梯度下降算法。你需要三个函数。</p>\n<ul>\n<li>compute_gradient实现上述式(4)和(5)</li>\n<li>上面的compute_cost实现方程(2)(代码来自以前的实验室)</li>\n<li>gradient_descent，使用compute_gradient和compute_cost</li>\n</ul>\n<p>Conventions:</p>\n<p><img src=\"/2023/03/13/ML003/image-20230313151857315.png\" alt=\"image-20230313151857315\"></p>\n<h2 id=\"compute-gradient\"><a href=\"#compute-gradient\" class=\"headerlink\" title=\"compute_gradient\"></a>compute_gradient</h2><p><img src=\"/2023/03/13/ML003/image-20230313151947363.png\" alt=\"image-20230313151947363\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">compute_gradient</span>(<span class=\"hljs-params\">x, y, w, b</span>):</span><br>    <span class=\"hljs-string\">&quot;&quot;&quot;</span><br><span class=\"hljs-string\">    Computes the gradient for linear regression</span><br><span class=\"hljs-string\">    Args:</span><br><span class=\"hljs-string\">    \tx (ndarray (m,)): Data, m examples</span><br><span class=\"hljs-string\">    \ty (ndarray (m,)): target values</span><br><span class=\"hljs-string\">    \tw,b (scalar)\t: model parameters</span><br><span class=\"hljs-string\">    Returns</span><br><span class=\"hljs-string\">    \tdj_dw (scalar): The gradient of the cost w.r.t. the parameters w</span><br><span class=\"hljs-string\">    \tdj_db (scalar): The gradient of the cost w.r.t. the parameter b </span><br><span class=\"hljs-string\">    &quot;&quot;&quot;</span><br>    <br>    <span class=\"hljs-comment\">#Number of training examples</span><br>    m = x.shape[<span class=\"hljs-number\">0</span>]<br>    dj_de = <span class=\"hljs-number\">0</span><br>    dj_db = <span class=\"hljs-number\">0</span><br>    <br>    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(m):<br>        f_wb = w * x[i] + b;<br>        dj_dw_i = (f_wb - y[i]) * x[i]<br>        dj_db_i = f_wb - y[i]<br>        dj_db += dj_db_i<br>        dj_dw += dj_dw_i<br>    dj_dw = dj_dw / m<br>    dj_db = dj_db / m<br>    <br>    <span class=\"hljs-keyword\">return</span> dj_dw, dj_db<br></code></pre></div></td></tr></table></figure>\n\n<p>课程描述了梯度下降如何利用在某一点上对参数代价的偏导数来更新该参数。</p>\n<p>让我们使用compute_gradient函数来查找并绘制代价函数相对于其中一个参数𝑤0的偏导数。</p>\n<p><img src=\"/2023/03/13/ML003/image-20230313154012173.png\" alt=\"image-20230313154012173\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">plt_gradients(x_train,y_train, compute_cost, compute_gradient)<br>plt.show()<br></code></pre></div></td></tr></table></figure>\n\n<p><img src=\"/2023/03/13/ML003/image-20230313154210478.png\" alt=\"image-20230313154210478\"></p>\n<p><img src=\"/2023/03/13/ML003/image-20230313154333429.png\" alt=\"image-20230313154333429\"></p>\n<h2 id=\"Gradient-Descent\"><a href=\"#Gradient-Descent\" class=\"headerlink\" title=\"Gradient Descent\"></a>Gradient Descent</h2><p>现在可以计算梯度，上面公式(3)中描述的梯度下降可以在下面的gradient_descent中实现。注释中描述了实现的细节。下面，您将利用这个函数在训练数据上找到w和b的最佳值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">gradient_descent</span>(<span class=\"hljs-params\">x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function</span>):</span> <br>    <span class=\"hljs-string\">&quot;&quot;&quot;</span><br><span class=\"hljs-string\">    Performs gradient descent to fit w,b. Updates w,b by taking </span><br><span class=\"hljs-string\">    num_iters gradient steps with learning rate alpha</span><br><span class=\"hljs-string\">    </span><br><span class=\"hljs-string\">    Args:</span><br><span class=\"hljs-string\">      x (ndarray (m,))  : Data, m examples </span><br><span class=\"hljs-string\">      y (ndarray (m,))  : target values</span><br><span class=\"hljs-string\">      w_in,b_in (scalar): initial values of model parameters  </span><br><span class=\"hljs-string\">      alpha (float):     Learning rate</span><br><span class=\"hljs-string\">      num_iters (int):   number of iterations to run gradient descent</span><br><span class=\"hljs-string\">      cost_function:     function to call to produce cost</span><br><span class=\"hljs-string\">      gradient_function: function to call to produce gradient</span><br><span class=\"hljs-string\">      </span><br><span class=\"hljs-string\">    Returns:</span><br><span class=\"hljs-string\">      w (scalar): Updated value of parameter after running gradient descent</span><br><span class=\"hljs-string\">      b (scalar): Updated value of parameter after running gradient descent</span><br><span class=\"hljs-string\">      J_history (List): History of cost values</span><br><span class=\"hljs-string\">      p_history (list): History of parameters [w,b] </span><br><span class=\"hljs-string\">      &quot;&quot;&quot;</span><br>    <br>    w = copy.deepcopy(w_in) <span class=\"hljs-comment\"># avoid modifying global w_in</span><br>    <span class=\"hljs-comment\"># An array to store cost J and w&#x27;s at each iteration primarily for graphing later</span><br>    J_history = []<br>    p_history = []<br>    b = b_in<br>    w = w_in<br>    <br>    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(num_iters):<br>        <span class=\"hljs-comment\"># Calculate the gradient and update the parameters using gradient_function</span><br>        dj_dw, dj_db = gradient_function(x, y, w , b)     <br><br>        <span class=\"hljs-comment\"># Update Parameters using equation (3) above</span><br>        b = b - alpha * dj_db                            <br>        w = w - alpha * dj_dw                            <br><br>        <span class=\"hljs-comment\"># Save cost J at each iteration</span><br>        <span class=\"hljs-keyword\">if</span> i&lt;<span class=\"hljs-number\">100000</span>:      <span class=\"hljs-comment\"># prevent resource exhaustion </span><br>            J_history.append( cost_function(x, y, w , b))<br>            p_history.append([w,b])<br>        <span class=\"hljs-comment\"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span><br>        <span class=\"hljs-keyword\">if</span> i% math.ceil(num_iters/<span class=\"hljs-number\">10</span>) == <span class=\"hljs-number\">0</span>:<br>            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;Iteration <span class=\"hljs-subst\">&#123;i:<span class=\"hljs-number\">4</span>&#125;</span>: Cost <span class=\"hljs-subst\">&#123;J_history[-<span class=\"hljs-number\">1</span>]:<span class=\"hljs-number\">0.2</span>e&#125;</span> &quot;</span>,<br>                  <span class=\"hljs-string\">f&quot;dj_dw: <span class=\"hljs-subst\">&#123;dj_dw: <span class=\"hljs-number\">0.3</span>e&#125;</span>, dj_db: <span class=\"hljs-subst\">&#123;dj_db: <span class=\"hljs-number\">0.3</span>e&#125;</span>  &quot;</span>,<br>                  <span class=\"hljs-string\">f&quot;w: <span class=\"hljs-subst\">&#123;w: <span class=\"hljs-number\">0.3</span>e&#125;</span>, b:<span class=\"hljs-subst\">&#123;b: <span class=\"hljs-number\">0.5</span>e&#125;</span>&quot;</span>)<br> <br>    <span class=\"hljs-keyword\">return</span> w, b, J_history, p_history <span class=\"hljs-comment\">#return w and J,w history for graphing</span><br></code></pre></div></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">Iteration    <span class=\"hljs-number\">0</span>: Cost <span class=\"hljs-number\">7.93e+04</span>  dj_dw: -<span class=\"hljs-number\">6.500e+02</span>, dj_db: -<span class=\"hljs-number\">4.000e+02</span>   w:  <span class=\"hljs-number\">6.500e+00</span>, b: <span class=\"hljs-number\">4.00000e+00</span><br>Iteration <span class=\"hljs-number\">1000</span>: Cost <span class=\"hljs-number\">3.41e+00</span>  dj_dw: -<span class=\"hljs-number\">3.712e-01</span>, dj_db:  <span class=\"hljs-number\">6.007e-01</span>   w:  <span class=\"hljs-number\">1.949e+02</span>, b: <span class=\"hljs-number\">1.08228e+02</span><br>Iteration <span class=\"hljs-number\">2000</span>: Cost <span class=\"hljs-number\">7.93e-01</span>  dj_dw: -<span class=\"hljs-number\">1.789e-01</span>, dj_db:  <span class=\"hljs-number\">2.895e-01</span>   w:  <span class=\"hljs-number\">1.975e+02</span>, b: <span class=\"hljs-number\">1.03966e+02</span><br>Iteration <span class=\"hljs-number\">3000</span>: Cost <span class=\"hljs-number\">1.84e-01</span>  dj_dw: -<span class=\"hljs-number\">8.625e-02</span>, dj_db:  <span class=\"hljs-number\">1.396e-01</span>   w:  <span class=\"hljs-number\">1.988e+02</span>, b: <span class=\"hljs-number\">1.01912e+02</span><br>Iteration <span class=\"hljs-number\">4000</span>: Cost <span class=\"hljs-number\">4.28e-02</span>  dj_dw: -<span class=\"hljs-number\">4.158e-02</span>, dj_db:  <span class=\"hljs-number\">6.727e-02</span>   w:  <span class=\"hljs-number\">1.994e+02</span>, b: <span class=\"hljs-number\">1.00922e+02</span><br>Iteration <span class=\"hljs-number\">5000</span>: Cost <span class=\"hljs-number\">9.95e-03</span>  dj_dw: -<span class=\"hljs-number\">2.004e-02</span>, dj_db:  <span class=\"hljs-number\">3.243e-02</span>   w:  <span class=\"hljs-number\">1.997e+02</span>, b: <span class=\"hljs-number\">1.00444e+02</span><br>Iteration <span class=\"hljs-number\">6000</span>: Cost <span class=\"hljs-number\">2.31e-03</span>  dj_dw: -<span class=\"hljs-number\">9.660e-03</span>, dj_db:  <span class=\"hljs-number\">1.563e-02</span>   w:  <span class=\"hljs-number\">1.999e+02</span>, b: <span class=\"hljs-number\">1.00214e+02</span><br>Iteration <span class=\"hljs-number\">7000</span>: Cost <span class=\"hljs-number\">5.37e-04</span>  dj_dw: -<span class=\"hljs-number\">4.657e-03</span>, dj_db:  <span class=\"hljs-number\">7.535e-03</span>   w:  <span class=\"hljs-number\">1.999e+02</span>, b: <span class=\"hljs-number\">1.00103e+02</span><br>Iteration <span class=\"hljs-number\">8000</span>: Cost <span class=\"hljs-number\">1.25e-04</span>  dj_dw: -<span class=\"hljs-number\">2.245e-03</span>, dj_db:  <span class=\"hljs-number\">3.632e-03</span>   w:  <span class=\"hljs-number\">2.000e+02</span>, b: <span class=\"hljs-number\">1.00050e+02</span><br>Iteration <span class=\"hljs-number\">9000</span>: Cost <span class=\"hljs-number\">2.90e-05</span>  dj_dw: -<span class=\"hljs-number\">1.082e-03</span>, dj_db:  <span class=\"hljs-number\">1.751e-03</span>   w:  <span class=\"hljs-number\">2.000e+02</span>, b: <span class=\"hljs-number\">1.00024e+02</span><br>(w,b) found by gradient descent: (<span class=\"hljs-number\">199.9929</span>,<span class=\"hljs-number\">100.0116</span>)<br></code></pre></div></td></tr></table></figure>\n\n<p>花点时间，注意上面打印的梯度下降过程的一些特征。</p>\n<ul>\n<li>正如课堂上的幻灯片所描述的，成本开始很大，然后迅速下降。</li>\n<li>偏导数dj_dw和dj_db也变小了，起初很快，然后变慢。正如课堂上的图表所示，随着过程接近“碗底”，由于在这一点上的导数值较小，进程会变慢。</li>\n<li>尽管学习率alpha保持不变，但进程会减慢</li>\n</ul>\n<p><img src=\"/2023/03/13/ML003/image-20230313155614815.png\" alt=\"image-20230313155614815\"></p>\n<h2 id=\"Cost-versus-iterations-of-gradient-descent\"><a href=\"#Cost-versus-iterations-of-gradient-descent\" class=\"headerlink\" title=\"Cost versus iterations of gradient descent\"></a>Cost versus iterations of gradient descent</h2><p>成本与迭代的关系图是衡量梯度下降技术进展的有用方法。在成功的运行中，成本总是会降低。最初成本的变化如此之快，用不同于最终下降的尺度来描绘最初的上升是有用的。</p>\n<p>在下面的图表中，请注意轴上的成本规模和迭代步骤。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># plot cost versus iteration  </span><br>fig, (ax1, ax2) = plt.subplots(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>, constrained_layout=<span class=\"hljs-literal\">True</span>, figsize=(<span class=\"hljs-number\">12</span>,<span class=\"hljs-number\">4</span>))<br>ax1.plot(J_hist[:<span class=\"hljs-number\">100</span>])<br>ax2.plot(<span class=\"hljs-number\">1000</span> + np.arange(<span class=\"hljs-built_in\">len</span>(J_hist[<span class=\"hljs-number\">1000</span>:])), J_hist[<span class=\"hljs-number\">1000</span>:])<br>ax1.set_title(<span class=\"hljs-string\">&quot;Cost vs. iteration(start)&quot;</span>);  ax2.set_title(<span class=\"hljs-string\">&quot;Cost vs. iteration (end)&quot;</span>)<br>ax1.set_ylabel(<span class=\"hljs-string\">&#x27;Cost&#x27;</span>)            ;  ax2.set_ylabel(<span class=\"hljs-string\">&#x27;Cost&#x27;</span>) <br>ax1.set_xlabel(<span class=\"hljs-string\">&#x27;iteration step&#x27;</span>)  ;  ax2.set_xlabel(<span class=\"hljs-string\">&#x27;iteration step&#x27;</span>) <br>plt.show()<br></code></pre></div></td></tr></table></figure>\n\n<p><img src=\"/2023/03/13/ML003/image-20230313160112122.png\" alt=\"image-20230313160112122\"></p>\n<h2 id=\"Predictions\"><a href=\"#Predictions\" class=\"headerlink\" title=\"Predictions\"></a>Predictions</h2><p>现在您已经发现了参数𝑤的最佳值和𝑏，您现在可以使用该模型根据我们学习的参数来预测房屋价值。正如预期的那样，在相同的住房条件下，预测值与训练值几乎相同。进一步，不在预测中的值与期望值一致。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;1000 sqft house prediction <span class=\"hljs-subst\">&#123;w_final*<span class=\"hljs-number\">1.0</span> + b_final:<span class=\"hljs-number\">0.1</span>f&#125;</span> Thousand dollars&quot;</span>)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;1200 sqft house prediction <span class=\"hljs-subst\">&#123;w_final*<span class=\"hljs-number\">1.2</span> + b_final:<span class=\"hljs-number\">0.1</span>f&#125;</span> Thousand dollars&quot;</span>)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;2000 sqft house prediction <span class=\"hljs-subst\">&#123;w_final*<span class=\"hljs-number\">2.0</span> + b_final:<span class=\"hljs-number\">0.1</span>f&#125;</span> Thousand dollars&quot;</span>)<br></code></pre></div></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-number\">1000</span> sqft house prediction <span class=\"hljs-number\">300.0</span> Thousand dollars<br><span class=\"hljs-number\">1200</span> sqft house prediction <span class=\"hljs-number\">340.0</span> Thousand dollars<br><span class=\"hljs-number\">2000</span> sqft house prediction <span class=\"hljs-number\">500.0</span> Thousand dollars<br></code></pre></div></td></tr></table></figure>\n\n<h2 id=\"Plotting\"><a href=\"#Plotting\" class=\"headerlink\" title=\"Plotting\"></a>Plotting</h2><p>您可以通过在代价(w,b)的等高线图上绘制迭代的代价来显示梯度下降在执行过程中的进度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">fig, ax = plt.subplots(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>, figsize=(<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">6</span>))<br>plt_contour_wgrad(x_train, y_train, p_hist, ax)<br></code></pre></div></td></tr></table></figure>\n\n<p><img src=\"/2023/03/13/ML003/image-20230313160446994.png\" alt=\"image-20230313160446994\"></p>\n<p>上面的等高线图显示了𝑤和𝑏范围内的𝑐𝑜𝑠𝑡(𝑤，𝑏)。成本水平由圆环表示。用红色箭头覆盖的是梯度下降的路径。这里有一些需要注意的事情:</p>\n<ul>\n<li>这条路径朝着它的目标稳步(单调)前进。</li>\n<li>最初的步骤比接近目标的步骤要大得多。</li>\n</ul>\n<p>放大，我们可以看到梯度下降的最后步骤。注意，阶梯之间的距离随着梯度趋近于零而缩小。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">fig, ax = plt.subplots(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>, figsize=(<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">4</span>))<br>plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[<span class=\"hljs-number\">180</span>, <span class=\"hljs-number\">220</span>, <span class=\"hljs-number\">0.5</span>], b_range=[<span class=\"hljs-number\">80</span>, <span class=\"hljs-number\">120</span>, <span class=\"hljs-number\">0.5</span>],<br>            contours=[<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">5</span>,<span class=\"hljs-number\">10</span>,<span class=\"hljs-number\">20</span>],resolution=<span class=\"hljs-number\">0.5</span>)<br></code></pre></div></td></tr></table></figure>\n\n<p><img src=\"/2023/03/13/ML003/image-20230313160944244.png\" alt=\"image-20230313160944244\"></p>\n<h2 id=\"Increased-Learning-Rate\"><a href=\"#Increased-Learning-Rate\" class=\"headerlink\" title=\"Increased Learning Rate\"></a>Increased Learning Rate</h2><p>在这节课中，在式(3)中有一个关于学习率的合适值𝛼的讨论。𝛼越大，梯度下降收敛到解的速度就越快。但是，如果它太大，梯度下降就会发散。上面有一个很好收敛的解的例子。让我们试着增加𝛼的值看看会发生什么:</p>\n<p><img src=\"/2023/03/13/ML003/image-20230313161516209.png\" alt=\"image-20230313161516209\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># initialize parameters</span><br>w_init = <span class=\"hljs-number\">0</span><br>b_init = <span class=\"hljs-number\">0</span><br><span class=\"hljs-comment\"># set alpha to a large value</span><br>iterations = <span class=\"hljs-number\">10</span><br>tmp_alpha = <span class=\"hljs-number\">8.0e-1</span><br><span class=\"hljs-comment\"># run gradient descent</span><br>w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, <br>                                                    iterations, compute_cost, compute_gradient)<br></code></pre></div></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs pyhton\">Iteration    0: Cost 2.58e+05  dj_dw: -6.500e+02, dj_db: -4.000e+02   w:  5.200e+02, b: 3.20000e+02<br>Iteration    1: Cost 7.82e+05  dj_dw:  1.130e+03, dj_db:  7.000e+02   w: -3.840e+02, b:-2.40000e+02<br>Iteration    2: Cost 2.37e+06  dj_dw: -1.970e+03, dj_db: -1.216e+03   w:  1.192e+03, b: 7.32800e+02<br>Iteration    3: Cost 7.19e+06  dj_dw:  3.429e+03, dj_db:  2.121e+03   w: -1.551e+03, b:-9.63840e+02<br>Iteration    4: Cost 2.18e+07  dj_dw: -5.974e+03, dj_db: -3.691e+03   w:  3.228e+03, b: 1.98886e+03<br>Iteration    5: Cost 6.62e+07  dj_dw:  1.040e+04, dj_db:  6.431e+03   w: -5.095e+03, b:-3.15579e+03<br>Iteration    6: Cost 2.01e+08  dj_dw: -1.812e+04, dj_db: -1.120e+04   w:  9.402e+03, b: 5.80237e+03<br>Iteration    7: Cost 6.09e+08  dj_dw:  3.156e+04, dj_db:  1.950e+04   w: -1.584e+04, b:-9.80139e+03<br>Iteration    8: Cost 1.85e+09  dj_dw: -5.496e+04, dj_db: -3.397e+04   w:  2.813e+04, b: 1.73730e+04<br>Iteration    9: Cost 5.60e+09  dj_dw:  9.572e+04, dj_db:  5.916e+04   w: -4.845e+04, b:-2.99567e+04<br></code></pre></div></td></tr></table></figure>\n\n<p><img src=\"/2023/03/13/ML003/image-20230313161636772.png\" alt=\"image-20230313161636772\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">plt_divergence(p_hist, J_hist,x_train, y_train)<br>plt.show()<br></code></pre></div></td></tr></table></figure>\n\n<p><img src=\"/2023/03/13/ML003/image-20230313161959222.png\" alt=\"image-20230313161959222\"></p>\n<p>上图中，左图显示了𝑤在梯度下降的前几个步骤中的进展。𝑤从正振荡到负，成本迅速增长。梯度下降同时在𝑤和𝑏上运行，所以需要右边的3d图才能看到完整的图片。</p>\n<h2 id=\"Congratulations\"><a href=\"#Congratulations\" class=\"headerlink\" title=\"Congratulations!\"></a>Congratulations!</h2><p>在这个实验室里，你:</p>\n<ul>\n<li>深入研究单个变量的梯度下降的细节。</li>\n<li>开发了一个计算梯度的程序</li>\n<li>看看梯度是什么</li>\n<li>完成一个梯度下降程序</li>\n<li>利用梯度下降法寻找参数</li>\n<li>检查了学习率大小的影响</li>\n</ul>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p><a href=\"https://www.bilibili.com/video/BV1Pa411X76s?p=5&amp;vd_source=3ae32e36058f58c5b85935fca9b77797\">https://www.bilibili.com/video/BV1Pa411X76s?p=5&amp;vd_source=3ae32e36058f58c5b85935fca9b77797</a></p>\n<p><a href=\"https://github.com/kaieye/2022-Machine-Learning-Specialization\">kaieye&#x2F;2022-Machine-Learning-Specialization (github.com)</a></p>\n",
            "tags": [
                "Tensorflow",
                "Machine Learning"
            ]
        },
        {
            "id": "https://xingyuanjie.top/2023/03/12/ML002/",
            "url": "https://xingyuanjie.top/2023/03/12/ML002/",
            "title": "代价函数",
            "date_published": "2023-03-12T08:33:17.000Z",
            "content_html": "<h2 id=\"代价函数\"><a href=\"#代价函数\" class=\"headerlink\" title=\"代价函数\"></a>代价函数</h2><p><img src=\"/2023/03/12/ML002/image-20230312164306128.png\" alt=\"image-20230312164306128\"></p>\n<h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><p>在本实验中，你将:</p>\n<ul>\n<li>你将实现和探索成本函数的线性回归伴随一个变量。</li>\n</ul>\n<h2 id=\"工具\"><a href=\"#工具\" class=\"headerlink\" title=\"工具\"></a>工具</h2><p>在本实验室中，我们将使用:</p>\n<ul>\n<li>NumPy，一个用于科学计算的流行库</li>\n<li>Matplotlib，用于绘制数据的流行库</li>\n<li>本地目录的lab_utils_uni.py文件中的本地绘图例程</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">import</span> nunpy <span class=\"hljs-keyword\">as</span> np<br>%matplotlib widget<br><span class=\"hljs-keyword\">import</span> matplotlib.pyplot <span class=\"hljs-keyword\">as</span> plt<br><span class=\"hljs-keyword\">from</span> lab_utils_uni <span class=\"hljs-keyword\">import</span> plt_intuition, plt_stationary, plt_updata_onclick, soup_bowl<br>plt.style.use(<span class=\"hljs-string\">&#x27;./deeplearning.mplstyle&#x27;</span>)<br></code></pre></div></td></tr></table></figure>\n\n<h2 id=\"问题意境\"><a href=\"#问题意境\" class=\"headerlink\" title=\"问题意境\"></a>问题意境</h2><p>你想要一个模型，它可以根据房子的大小预测房价。让我们使用与上一个实验室之前相同的两个数据点——一个1000平方英尺的房子卖了30万美元，一个2000平方英尺的房子卖了50万美元。</p>\n<table>\n<thead>\n<tr>\n<th>Size(1000 sqft)</th>\n<th>Price(1000s of dollars)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>300</td>\n</tr>\n<tr>\n<td>2</td>\n<td>500</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">x_train = np.array([<span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">2.0</span>])   <span class=\"hljs-comment\">#(size in 1000 square feet)</span><br>y_train = np.zrray([<span class=\"hljs-number\">300.0</span>, <span class=\"hljs-number\">500.0</span>])\t<span class=\"hljs-comment\">#(price in 1000s of dollars)</span><br></code></pre></div></td></tr></table></figure>\n\n<h2 id=\"计算代价\"><a href=\"#计算代价\" class=\"headerlink\" title=\"计算代价\"></a>计算代价</h2><p>这个作业中的术语“成本”可能会让人有点困惑，因为数据是住房成本。在这里，成本是衡量我们的模型预测房子目标价格的好坏。“价格”一词指的是住房数据。</p>\n<p>含一个变量的成本方程为:</p>\n<p><img src=\"/2023/03/12/ML002/image-20230312165434160.png\" alt=\"image-20230312165434160\"></p>\n<p>在这里</p>\n<p><img src=\"/2023/03/12/ML002/image-20230312165503613.png\" alt=\"image-20230312165503613\"></p>\n<ul>\n<li>f_w,b(x^i)是我们使用参数w,b来预测例子i。</li>\n<li>（f_w,b(x^i) - y^i）^2  是目标值与预测值之间的差的平方</li>\n<li>这些差异被加在所有m例子上，再除以2m，得到代价函数 <strong>J（w,b）</strong></li>\n</ul>\n<p>注意，在讲座中，总和的范围通常是从1到m，而代码将从0到m-1。</p>\n<p>下面的代码通过遍历每个示例来计算成本。在每个循环中:</p>\n<ul>\n<li>f_wb，计算一个预测</li>\n<li>目标和预测之间的差值被计算和平方。</li>\n<li>这被加到总成本中。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">compute_cost</span>(<span class=\"hljs-params\">x, y, w, b</span>):</span><br>    <span class=\"hljs-string\">&quot;&quot;&quot;</span><br><span class=\"hljs-string\">    Computes the cost function for linear regression</span><br><span class=\"hljs-string\">    </span><br><span class=\"hljs-string\">    Args:</span><br><span class=\"hljs-string\">    \tx (ndarray (m,)):Data, m examples</span><br><span class=\"hljs-string\">    \ty (ndarray (m,)):target values</span><br><span class=\"hljs-string\">    \tw,b (scalar)\t:model parameters</span><br><span class=\"hljs-string\">    </span><br><span class=\"hljs-string\">    Returns</span><br><span class=\"hljs-string\">    \ttotal_cost (float):The cost of using w,b as the parameters for linear regression to fit the data points in x and y</span><br><span class=\"hljs-string\">    &quot;&quot;&quot;</span><br>    <span class=\"hljs-comment\">#number of training examples</span><br>    m = x.shape[<span class=\"hljs-number\">0</span>]<br>    <br>    cost_sum = <span class=\"hljs-number\">0</span><br>    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(m):<br>        f_wb = w * x[i] + b<br>        cost = (f_wb - y[i])**<span class=\"hljs-number\">2</span><br>        cost_sum = cost_sum + cost<br>    total_cost = (<span class=\"hljs-number\">1</span>/(<span class=\"hljs-number\">2</span>*m)) * cost_sum<br>    <br>    <span class=\"hljs-keyword\">return</span> total_cost<br></code></pre></div></td></tr></table></figure>\n\n<h2 id=\"Cost-Function-Intuition\"><a href=\"#Cost-Function-Intuition\" class=\"headerlink\" title=\"Cost Function Intuition\"></a>Cost Function Intuition</h2><p><img src=\"/2023/03/12/ML002/image-20230312172325465.png\" alt=\"image-20230312172325465\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">plt_intuition(x_train, y_train)<br></code></pre></div></td></tr></table></figure>\n\n<p><img src=\"/2023/03/12/ML002/image-20230313144221393.png\" alt=\"image-20230313144221393\"></p>\n<p>情节中有几点值得一提。</p>\n<ul>\n<li>当𝑤&#x3D;200时，成本最小化，这与之前实验室的结果相吻合。</li>\n<li>因为在成本方程中，目标和预测之间的差异是平方，当𝑤时，成本迅速增加不是太大就是太小。</li>\n<li>使用通过最小化成本选择的w和b，可以得到与数据完美匹配的直线。</li>\n</ul>\n<h2 id=\"Cost-Function-Visualiztion-3D\"><a href=\"#Cost-Function-Visualiztion-3D\" class=\"headerlink\" title=\"Cost Function Visualiztion-3D\"></a>Cost Function Visualiztion-3D</h2><p>你可以通过三维绘图或等高线图看到成本是如何随w和b变化的。</p>\n<p>值得注意的是，这门课的一些情节会变得相当复杂。本文提供了绘图例程，虽然通读代码以熟悉这些方法是有指导意义的，但要成功完成课程并不需要这样做。例程在本地目录lab_utils_uni.py中。</p>\n<h2 id=\"Larger-Data-Set\"><a href=\"#Larger-Data-Set\" class=\"headerlink\" title=\"Larger Data Set\"></a>Larger Data Set</h2><p>较大的数据集用更多的数据点来观察一个场景是很有指导意义的。该数据集包括不在同一线上的数据点。这对成本方程意味着什么?我们能找到𝑤、𝑏那样使得代价是0?</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">x_train = np.array([<span class=\"hljs-number\">1.0</span>,<span class=\"hljs-number\">1.7</span>,<span class=\"hljs-number\">2.0</span>,<span class=\"hljs-number\">2.5</span>,<span class=\"hljs-number\">3.0</span>,<span class=\"hljs-number\">3.2</span>])<br>y_train = np.array([<span class=\"hljs-number\">250</span>,<span class=\"hljs-number\">300</span>,<span class=\"hljs-number\">480</span>,<span class=\"hljs-number\">430</span>,<span class=\"hljs-number\">630</span>,<span class=\"hljs-number\">730</span>])<br></code></pre></div></td></tr></table></figure>\n\n<p>在等高线图中，点击一个点，选择w和b，以达到最低的成本。使用轮廓来指导你的选择。注意，更新图形可能需要几秒钟的时间。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">plt.close(<span class=\"hljs-string\">&#x27;all&#x27;</span>)<br>fig, ax ,dyn_items = plt_stationary(x_train, y_train)<br>updater = plt_update_onclick(fig, ax, x_train,y_train,dyn_items)<br></code></pre></div></td></tr></table></figure>\n\n<p>上面，注意左边图中的虚线。这些代表了你的训练集中每个例子所贡献的代价的部分。在本例中，值约为𝑤&#x3D;209和𝑏&#x3D; 2.4提供低代价。请注意，因为我们的训练示例不在一条线上，所以最小代价不为零。</p>\n<h2 id=\"Convex-Cost-surface\"><a href=\"#Convex-Cost-surface\" class=\"headerlink\" title=\"Convex Cost surface\"></a>Convex Cost surface</h2><p>成本函数平方损失的事实确保了“误差曲面”像汤碗一样凸出。它总是有一个最小值，可以通过在所有维度上跟随梯度来达到。在前面的图中，因为𝑤和𝑏尺寸比例不同，这是不容易识别的。下图，其中𝑤和𝑏都是对称的，在讲座中展示过:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">soup_bowl()<br></code></pre></div></td></tr></table></figure>\n\n<h2 id=\"Congratulations\"><a href=\"#Congratulations\" class=\"headerlink\" title=\"Congratulations!\"></a>Congratulations!</h2><p>您已经学习了以下内容:</p>\n<ul>\n<li>成本方程提供了一种衡量预测与训练数据匹配程度的方法。</li>\n<li>最小化成本可以提供𝑤和b的最优值。</li>\n</ul>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p><a href=\"https://www.bilibili.com/video/BV1Pa411X76s?p=5&amp;vd_source=3ae32e36058f58c5b85935fca9b77797\">https://www.bilibili.com/video/BV1Pa411X76s?p=5&amp;vd_source=3ae32e36058f58c5b85935fca9b77797</a></p>\n<p><a href=\"https://github.com/kaieye/2022-Machine-Learning-Specialization\">kaieye&#x2F;2022-Machine-Learning-Specialization (github.com)</a></p>\n",
            "tags": [
                "Tensorflow",
                "Machine Learning"
            ]
        },
        {
            "id": "https://xingyuanjie.top/2023/03/06/ML001/",
            "url": "https://xingyuanjie.top/2023/03/06/ML001/",
            "title": "线性回归模型",
            "date_published": "2023-03-06T11:00:34.000Z",
            "content_html": "<h2 id=\"线性回归模型\"><a href=\"#线性回归模型\" class=\"headerlink\" title=\"线性回归模型\"></a>线性回归模型</h2><h3 id=\"MOdel-Representation\"><a href=\"#MOdel-Representation\" class=\"headerlink\" title=\"MOdel Representation\"></a>MOdel Representation</h3><p><img src=\"/2023/03/06/ML001/image-20230306195956356.png\" alt=\"image-20230306195956356\"></p>\n<h3 id=\"Goals\"><a href=\"#Goals\" class=\"headerlink\" title=\"Goals\"></a>Goals</h3><p>In this lab you will:</p>\n<ul>\n<li>learn to implement the model f_{w,b} for linear regression with one variable</li>\n</ul>\n<h3 id=\"Notation\"><a href=\"#Notation\" class=\"headerlink\" title=\"Notation\"></a>Notation</h3><p>Here is a summary of some of the notation you will encounter.</p>\n<p><img src=\"/2023/03/06/ML001/image-20230306200250050.png\" alt=\"image-20230306200250050\"></p>\n<h3 id=\"Tools\"><a href=\"#Tools\" class=\"headerlink\" title=\"Tools\"></a>Tools</h3><p>In this lab you will make use of:</p>\n<ul>\n<li><p>NumPy,a popular library for scientific computing</p>\n</li>\n<li><p>Matplotlib,a popular library for plotting data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np<br><span class=\"hljs-keyword\">import</span> matplotlib.pyplot <span class=\"hljs-keyword\">as</span> plt<br>plt.style.use(<span class=\"hljs-string\">&#x27;./deeplearning.mpstyle&#x27;</span>)<br></code></pre></div></td></tr></table></figure></li>\n</ul>\n<h3 id=\"Problem-Statement\"><a href=\"#Problem-Statement\" class=\"headerlink\" title=\"Problem Statement\"></a>Problem Statement</h3><p><img src=\"/2023/03/06/ML001/image-20230306200642937.png\" alt=\"image-20230306200642937\"></p>\n<p>As in the lecture,you will use the motivating example of housing price prediction. This lab will use a simple data set with only two data points - a house with 1000 square feet(sqft) sold for $300,000 and a house with 2000 square feet sold for $500,000.These two points will constitute our data or training set. In this lab, the units of size are 1000 sqft and the units of price are 1000s of dollars.</p>\n<table>\n<thead>\n<tr>\n<th>Size (1000 sqft)</th>\n<th>Price (1000s of dollars)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1.0</td>\n<td>300</td>\n</tr>\n<tr>\n<td>2.0</td>\n<td>500</td>\n</tr>\n</tbody></table>\n<p>You would like to fit a linear regression model(shown above as the blue straight line)through these two points, so you can then predict price for other houses - say, a house with 1200 sqft.</p>\n<p>Please run the following code cell to create your x_train and y_train variables. The data is stored in one-dimensional NumPy arrays.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\">#x_train is the input variable (size in 1000 square feet)</span><br><span class=\"hljs-comment\">#y_train is the target (price in 1000s of dollars)</span><br>x_train = np.array([<span class=\"hljs-number\">1.0</span>,<span class=\"hljs-number\">2.0</span>])<br>y_train = np.array([<span class=\"hljs-number\">300.0</span>,<span class=\"hljs-number\">500.0</span>])<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;x_train = <span class=\"hljs-subst\">&#123;x_train&#125;</span>&quot;</span>)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;y_train = <span class=\"hljs-subst\">&#123;y_train&#125;</span>&quot;</span>)<br></code></pre></div></td></tr></table></figure>\n\n<h3 id=\"Number-of-training-examples-m\"><a href=\"#Number-of-training-examples-m\" class=\"headerlink\" title=\"Number of training examples m\"></a>Number of training examples m</h3><p>you will use m to denote the number of training examples. Numpy arrays have a .shape parameter. x_train.shape return a python tuple with an entry for each dimension. x_train.shape[0] is the length of the array and number of examples as shown below.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># m is the number of training examples</span><br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;x_train.shape: <span class=\"hljs-subst\">&#123;x_train.shape&#125;</span>&quot;</span>)<br>m = x_train.shape[<span class=\"hljs-number\">0</span>]<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;Number of training example is: <span class=\"hljs-subst\">&#123;m&#125;</span>&quot;</span>)<br></code></pre></div></td></tr></table></figure>\n\n<p>x.train.shape: (2,)</p>\n<p>Number of training examples is: 2</p>\n<p><strong>One can also use the Python len() function as shown below.</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># m is the number of training examples</span><br>m = <span class=\"hljs-built_in\">len</span>(x_train)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;Number of training example is: <span class=\"hljs-subst\">&#123;m&#125;</span>&quot;</span>)<br></code></pre></div></td></tr></table></figure>\n\n<p>Number of training examples  is:\t2</p>\n<h3 id=\"Training-example-x-i-y-i\"><a href=\"#Training-example-x-i-y-i\" class=\"headerlink\" title=\"Training example x_i, y_i\"></a>Training example x_i, y_i</h3><p>You will use (x(𝑖), y(𝑖)) to denote the 𝑖(th) training example. Since Python is zero indexed, (x(0), y(0) is (1.0, 300.0) and (x(1), y(1) is (2.0, 500.0).</p>\n<p>To access a value in a Numpy array, one indexes the array with the desired offset. For example the syntax to access location zero of x_train is x_train[0]. Run the next code block below to get the i(th) training example.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">i = <span class=\"hljs-number\">0</span> <span class=\"hljs-comment\">#Change this to 1 to see (x^1,y^1)</span><br><br>x_i = x_train[i]<br>y_i = y_train[i]<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;(x^(<span class=\"hljs-subst\">&#123;i&#125;</span>), y^(<span class=\"hljs-subst\">&#123;i&#125;</span>)) = (<span class=\"hljs-subst\">&#123;x_i&#125;</span>, <span class=\"hljs-subst\">&#123;y_i&#125;</span>)&quot;</span>)<br></code></pre></div></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">(x^(<span class=\"hljs-number\">0</span>), y^(<span class=\"hljs-number\">0</span>)) = (<span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">300.0</span>)<br></code></pre></div></td></tr></table></figure>\n\n<h3 id=\"Plotting-the-data\"><a href=\"#Plotting-the-data\" class=\"headerlink\" title=\"Plotting the data\"></a>Plotting the data</h3><p>You can plot these two points using the scatter() function is the matplotlib library,as shown in the cell below.</p>\n<ul>\n<li>The function arguments marker and c show the points as red crosses(the default is blue dots.)</li>\n</ul>\n<p>You can use other functions in the matplotlib library to set title and labels to display.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\">#Plot the data points</span><br>plt.scatter(x_train, y_train, marker=<span class=\"hljs-string\">&#x27;x&#x27;</span> c=<span class=\"hljs-string\">&#x27;r&#x27;</span>)<br><span class=\"hljs-comment\">#Set the title</span><br>plt.title(<span class=\"hljs-string\">&quot;Housing Prices&quot;</span>)<br><span class=\"hljs-comment\">#Set the y-axis label</span><br>plt.ylabel(<span class=\"hljs-string\">&#x27;Price (in 1000s of dollars)&#x27;</span>)<br><span class=\"hljs-comment\">#Set the x-axis lbel</span><br>plt.xlabel(<span class=\"hljs-string\">&#x27;Size (1000 sqft)&#x27;</span>)<br>plt.show()<br></code></pre></div></td></tr></table></figure>\n\n<p><img src=\"/2023/03/06/ML001/image-20230306204822102.png\" alt=\"image-20230306204822102\"></p>\n<h3 id=\"Model-function\"><a href=\"#Model-function\" class=\"headerlink\" title=\"Model function\"></a>Model function</h3><p><img src=\"/2023/03/06/ML001/image-20230306204918585.png\" alt=\"image-20230306204918585\"></p>\n<p>As described in lecture, the model function for linear regression (which is a function that maps from x to y)is represented as</p>\n<p><img src=\"/2023/03/06/ML001/image-20230306205107527.png\" alt=\"image-20230306205107527\"></p>\n<p>The formula above is how you can represent straight lines - different values of w and b give you different straight lines on the plot.</p>\n<p>Let’s try to get a better intuition for this through the code blocks below. Let’s start with w &#x3D; 100 and b &#x3D;100.</p>\n<p>Note: You can come back to this cell to adjust the model’s w and b parameters.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">w = <span class=\"hljs-number\">100</span><br>b = <span class=\"hljs-number\">100</span><br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;w: <span class=\"hljs-subst\">&#123;w&#125;</span>&quot;</span>)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;b: <span class=\"hljs-subst\">&#123;b&#125;</span>&quot;</span>)<br></code></pre></div></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">w:\t<span class=\"hljs-number\">100</span><br>b:\t<span class=\"hljs-number\">100</span><br></code></pre></div></td></tr></table></figure>\n\n<p>Now,let’s compute the value of f_{w,b}(x^i) for your two data points. You can explicitly write this out for each data poins as -</p>\n<p>for x(0),f_wb &#x3D; w * x[0] + b</p>\n<p>for x(1),f_wb &#x3D; w * x[1] + b</p>\n<p>For a large number of data points, this can get unwieldy and repetitive. So instead, you can calculate the function output in a for loop as shown in the compute_model_output function below.</p>\n<p>Note:The argument description (ndarray (m,)) describes a Numpy n-dimensional array of shape (m,). (scalar) describes an argument without dimensions, just a magnitude.</p>\n<p>Note: np.zero(n) will return a one-dimensional numpy array with n entries</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">compute_model_output</span>(<span class=\"hljs-params\">x, w, b</span>):</span><br>    <span class=\"hljs-string\">&quot;&quot;&quot;</span><br><span class=\"hljs-string\">    Computes the prediction of a linear model</span><br><span class=\"hljs-string\">    Args:</span><br><span class=\"hljs-string\">    \tx (ndarray (m,)):Data, m examples</span><br><span class=\"hljs-string\">    \tw,b (scalar)\t:model parameters</span><br><span class=\"hljs-string\">    Returns</span><br><span class=\"hljs-string\">    \ty (ndarray (m,)):target values</span><br><span class=\"hljs-string\">    &quot;&quot;&quot;</span><br>    <br>    m = x.shape[<span class=\"hljs-number\">0</span>]<br>    f_wb = np.zeros(m)<br>    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(m)<br>    \tf_wb[i] = w * x[i] + b<br>    <br>    <span class=\"hljs-keyword\">return</span> f_wb<br></code></pre></div></td></tr></table></figure>\n\n<p>Now let’s call the compute_model_output function and plot the output.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">tmp_f_wb = compute_model_output(x_train, w, b,)<br><br><span class=\"hljs-comment\">#Plot our model prediction</span><br>plt.plot(x_train, tmp_f_wb, c=<span class=\"hljs-string\">&#x27;b&#x27;</span>,label=<span class=\"hljs-string\">&#x27;Our Prediction&#x27;</span>)<br><br><span class=\"hljs-comment\">#Plot the data points</span><br>plt.scatter(x_train, y_train, marker=<span class=\"hljs-string\">&#x27;x&#x27;</span>, c=<span class=\"hljs-string\">&#x27;r&#x27;</span>,label=<span class=\"hljs-string\">&#x27;Actual Values&#x27;</span>)<br><br><span class=\"hljs-comment\">#Set the title</span><br>plt.title(<span class=\"hljs-string\">&quot;Housing Prices&quot;</span>)<br><span class=\"hljs-comment\">#Set the y-axis label</span><br>plt.ylabel(<span class=\"hljs-string\">&#x27;Price (in 1000s of dollars)&#x27;</span>)<br><span class=\"hljs-comment\">#Set the x-axis label</span><br>plt.xlabel(<span class=\"hljs-string\">&#x27;Size (1000 sqft)&#x27;</span>)<br>plt.legend()<br>plt.show()<br></code></pre></div></td></tr></table></figure>\n\n<p><img src=\"/2023/03/06/ML001/image-20230306211337338.png\" alt=\"image-20230306211337338\"></p>\n<p>As you can see, setting w &#x3D; 100 and b &#x3D; 100 does not result in a line that fits our data.</p>\n<h3 id=\"Challenge\"><a href=\"#Challenge\" class=\"headerlink\" title=\"Challenge\"></a>Challenge</h3><p>Try experimenting with different values of w and b. What should the values be for a line that fits our data?</p>\n<p><strong>Tips:</strong></p>\n<p>You can use  your mouse to click on the triangle to the left of the green “Hints” below to reveal some hints for choosing b and w.</p>\n<p><strong>Hints</strong></p>\n<h3 id=\"Prediction\"><a href=\"#Prediction\" class=\"headerlink\" title=\"Prediction\"></a>Prediction</h3><p>Now that we have a model, we can use it to make our original prediction. Let’s predict the price of a house with 1200 sqft. Since the units of x are in 1000’s of sqft, x is 1.2.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">w = <span class=\"hljs-number\">200</span><br>b = <span class=\"hljs-number\">100</span><br>x_i = <span class=\"hljs-number\">1.2</span><br>cost_1200sqft = w * x_i + b<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f&quot;$<span class=\"hljs-subst\">&#123;cost_1200sqft:<span class=\"hljs-number\">.0</span>f&#125;</span> thousand dollars&quot;</span> )<br></code></pre></div></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter hljs\"><div class=\"hljs code-wrapper\"><pre><span class=\"line\">1</span><br></pre></div></td><td class=\"code\"><div class=\"hljs code-wrapper\"><pre><code class=\"hljs python\">$<span class=\"hljs-number\">340</span> thousand dollars<br></code></pre></div></td></tr></table></figure>\n\n<h3 id=\"Congratulations\"><a href=\"#Congratulations\" class=\"headerlink\" title=\"Congratulations!\"></a>Congratulations!</h3><p>In this lab you have learned:</p>\n<ul>\n<li>Linear regression bulids a model which establishes a relationship between features and targets</li>\n<li>In the example above, the feature was house size and the target was house price</li>\n<li>for simple linear regression, the model has two parameters w and b whose calue are ‘fit’ using training data.</li>\n<li>once a model’s parameters have been determined, the model can be used to make predictions on novel data.</li>\n</ul>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p><a href=\"https://www.bilibili.com/video/BV1Pa411X76s?p=5&amp;vd_source=3ae32e36058f58c5b85935fca9b77797\">https://www.bilibili.com/video/BV1Pa411X76s?p=5&amp;vd_source=3ae32e36058f58c5b85935fca9b77797</a></p>\n<p><a href=\"https://github.com/kaieye/2022-Machine-Learning-Specialization\">kaieye&#x2F;2022-Machine-Learning-Specialization (github.com)</a></p>\n",
            "tags": [
                "Tensorflow",
                "Machine Learning"
            ]
        }
    ]
}