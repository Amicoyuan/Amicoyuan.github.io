<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>Amicoyuan • Posts by &#34;gemv&#34; tag</title>
        <link>https://xingyuanjie.top</link>
        <description></description>
        <language>zh-CN</language>
        <pubDate>Mon, 22 May 2023 20:35:17 +0800</pubDate>
        <lastBuildDate>Mon, 22 May 2023 20:35:17 +0800</lastBuildDate>
        <category>AVX</category>
        <category>Data Structure</category>
        <category>String</category>
        <category>Find</category>
        <category>STL</category>
        <category>C++</category>
        <category>Set</category>
        <category>团体程序设计天梯赛</category>
        <category>Sort</category>
        <category>LinkList</category>
        <category>双向链表</category>
        <category>Graph</category>
        <category>邻接表</category>
        <category>Vector</category>
        <category>Binary tree</category>
        <category>BFS</category>
        <category>模拟</category>
        <category>邻接矩阵</category>
        <category>DFS</category>
        <category>结构体</category>
        <category>Double类型相等比较</category>
        <category>贪心</category>
        <category>并查集</category>
        <category>Map</category>
        <category>Pair</category>
        <category>CMU</category>
        <category>CSAPP</category>
        <category>CUDA</category>
        <category>Linux</category>
        <category>Tensorflow</category>
        <category>Machine Learning</category>
        <category>MPI</category>
        <category>Matrix</category>
        <category>GEMM</category>
        <category>AVX2</category>
        <category>blas</category>
        <category>gemm</category>
        <category>gemv</category>
        <category>dgemm</category>
        <category>Cache</category>
        <category>Blocking</category>
        <category>C/C++</category>
        <category>Intel</category>
        <category>LeetCode</category>
        <category>双指针</category>
        <category>哈希表</category>
        <category>数组翻转</category>
        <category>前缀和</category>
        <category>Travel</category>
        <category>Mirror</category>
        <category>牛客</category>
        <category>Numactl</category>
        <category>OpenMP</category>
        <category>register</category>
        <category>Slurm</category>
        <category>性能分析工具</category>
        <category>gcov</category>
        <category>bool</category>
        <item>
            <guid isPermalink="true">https://xingyuanjie.top/2023/05/22/blas/</guid>
            <title>BLAS（Basic Linear Algebra Subprograms）-基础线性代数子程序库</title>
            <link>https://xingyuanjie.top/2023/05/22/blas/</link>
            <category>blas</category>
            <category>gemm</category>
            <category>gemv</category>
            <pubDate>Mon, 22 May 2023 20:35:17 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;参考资料&#34;&gt;&lt;a href=&#34;#参考资料&#34; class=&#34;headerlink&#34; title=&#34;参考资料&#34;&gt;&lt;/a&gt;参考资料&lt;/h2&gt;&lt;h3 id=&#34;博客&#34;&gt;&lt;a href=&#34;#博客&#34; class=&#34;headerlink&#34; title=&#34;博客:&#34;&gt;&lt;/a&gt;博客:&lt;/h3&gt;&lt;p&gt;&lt;a href=&#34;https://www.jianshu.com/p/e01024892afb&#34;&gt;矩阵相乘在GPU上的终极优化：深度解析Maxas汇编器工作原理 - 简书 (jianshu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.leiphone.com/category/yanxishe/Puevv3ZWxn0heoEv.html&#34;&gt;OpenBLAS项目与矩阵乘法优化 | AI 研习社 | 雷峰网 (leiphone.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jcf94.com/2021/08/28/2021-08-28-simd/&#34;&gt;矩阵乘法与 SIMD | Chenfan Blog (jcf94.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhenhuaw.me/blog/2019/gemm-optimization.html&#34;&gt;通用矩阵乘（GEMM）优化算法 | 黎明灰烬 博客 (zhenhuaw.me)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/383115932&#34;&gt;大佬是怎么优雅实现矩阵乘法的？ - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/65436463&#34;&gt;OpenBLAS gemm从零入门 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://harleyszhang.github.io/cv_note/&#34;&gt;Introduction · cv算法工程师成长之路 (harleyszhang.github.io)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/435908830&#34;&gt;深入浅出GPU优化系列：GEMM优化（一） - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/410278370&#34;&gt;CUDA 矩阵乘法终极优化指南 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/368870275&#34;&gt;矩阵乘法的并行优化（1）：OPENMP、CUDA实现 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/617296073&#34;&gt;并行计算入门 UIUC ECE408 Lecture 7&amp;amp;8 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/143328317&#34;&gt;移动端arm cpu优化学习笔记第4弹–内联汇编入门 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/348372132&#34;&gt;C语言的内嵌汇编 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/105616727&#34;&gt;内嵌汇编学习 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/unbutun/article/details/6123472&#34;&gt;(88条消息) #define barrier() &lt;strong&gt;asm&lt;/strong&gt; &lt;strong&gt;volatile&lt;/strong&gt;(“”: : :”memory”) 中的memory是gcc的东西_unbutun的博客-CSDN博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_38669561/article/details/105192200?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control&#34;&gt;(88条消息) MIPS指令集：内嵌汇编asm语法介绍_daddu指令_无色云的博客-CSDN博客&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;论文：&#34;&gt;&lt;a href=&#34;#论文：&#34; class=&#34;headerlink&#34; title=&#34;论文：&#34;&gt;&lt;/a&gt;论文：&lt;/h3&gt;&lt;p&gt;&lt;a href=&#34;https://www.cs.utexas.edu/users/flame/FLAMEPublications.html&#34;&gt;Publications Related to the FLAME Project (utexas.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/1356052.1356053&#34;&gt;Anatomy of high-performance matrix multiplication | ACM Transactions on Mathematical Software&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3018743.3018755&#34;&gt;Understanding the GPU Microarchitecture to Achieve Bare-Metal Performance Tuning | Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/1804.06826&#34;&gt;1804.06826] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/6114452&#34;&gt;Fast implementation of DGEMM on Fermi GPU | IEEE Conference Publication | IEEE Xplore&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3369583.3393611&#34;&gt;High Performance is All about Minimizing Data Movement | Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing (acm.org)&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;社区-x2F-论坛&#34;&gt;&lt;a href=&#34;#社区-x2F-论坛&#34; class=&#34;headerlink&#34; title=&#34;社区&amp;#x2F;论坛:&#34;&gt;&lt;/a&gt;社区&amp;#x2F;论坛:&lt;/h3&gt;&lt;p&gt;&lt;a href=&#34;http://tvm.d2l.ai/chapter_gpu_schedules/vector_add.html#cuda-programming&#34;&gt;2. Vector Add — Dive into Deep Learning Compiler 0.1 documentation (d2l.ai)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html&#34;&gt;Intel® Intrinsics Guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/QNNPACK&#34;&gt;https://github.com/pytorch/QNNPACK&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/flame/blis&#34;&gt;https://github.com/flame/blis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mathematik.uni-ulm.de/~lehn/apfel/ulmBLAS/&#34;&gt;ulmBLAS (index) (uni-ulm.de)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mathematik.uni-ulm.de/~lehn/apfel/sghpc/gemm/index.html&#34;&gt;work&amp;#x2F;sghpc (index) (uni-ulm.de)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shpc.oden.utexas.edu/&#34;&gt;The Science of High-Performance Computing Group (utexas.edu)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/BBuf/how-to-optimize-gemm&#34;&gt;GitHub - BBuf&amp;#x2F;how-to-optimize-gemm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Liu-xiandong/How_to_optimize_in_GPU&#34;&gt;GitHub - Liu-xiandong&amp;#x2F;How_to_optimize_in_GPU: This is a series of GPU optimization topics. Here we will introduce how to optimize the CUDA kernel in detail. I will introduce several basic kernel optimizations, including: elementwise, reduce, sgemv, sgemm, etc. The performance of these kernels is basically at or near the theoretical limit.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-c-programming-guide/#&#34;&gt;CUDA C++ Programming Guide (nvidia.com)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/NervanaSystems/maxas/wiki/SGEMM&#34;&gt;SGEMM · NervanaSystems&amp;#x2F;maxas Wiki · GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Cjkkkk/CUDA_gemm&#34;&gt;GitHub - Cjkkkk&amp;#x2F;CUDA_gemm: A simple high performance CUDA GEMM implementation.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/yzhaiustc/Optimizing-SGEMM-on-NVIDIA-Turing-GPUs&#34;&gt;GitHub - yzhaiustc&amp;#x2F;Optimizing-SGEMM-on-NVIDIA-Turing-GPUs: Optimizing SGEMM kernel functions on NVIDIA GPUs to a close-to-cuBLAS performance.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/&#34;&gt;https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wiki.illinois.edu/wiki/display/ECE408/Class+Schedule&#34;&gt;Class Schedule - ECE408 - Illinois Wiki&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ibiblio.org/gferg/ldp/GCC-Inline-Assembly-HOWTO.html#toc2&#34;&gt;GCC-Inline-Assembly-HOWTO (ibiblio.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://missing-semester-cn.github.io/&#34;&gt;计算机教育中缺失的一课 · the missing semester of your cs education (missing-semester-cn.github.io)&lt;/a&gt;&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>
