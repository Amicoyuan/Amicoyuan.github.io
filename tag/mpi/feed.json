{
    "version": "https://jsonfeed.org/version/1",
    "title": "Amicoyuan • All posts by \"mpi\" tag",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2022/06/12/MPI%E5%AD%A6%E4%B9%A0(%E4%B8%80)-%E7%AE%80%E5%8D%95%E7%9A%84%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6/",
            "url": "http://example.com/2022/06/12/MPI%E5%AD%A6%E4%B9%A0(%E4%B8%80)-%E7%AE%80%E5%8D%95%E7%9A%84%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6/",
            "title": "MPI学习(一)-简单的发送接收",
            "date_published": "2022-06-12T07:11:29.651Z",
            "content_html": "<h1 id=\"MPI-简单的发送接收\"><a href=\"#MPI-简单的发送接收\" class=\"headerlink\" title=\"MPI-简单的发送接收\"></a>MPI-简单的发送接收</h1><p>打印来自进程问候语句的MPI程序</p>\n<h2 id=\"所使用的MPI原语\"><a href=\"#所使用的MPI原语\" class=\"headerlink\" title=\"所使用的MPI原语\"></a>所使用的MPI原语</h2><p><img src=\"/2022/06/12/MPI%E5%AD%A6%E4%B9%A0(%E4%B8%80)-%E7%AE%80%E5%8D%95%E7%9A%84%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6/mpi001.png\"></p>\n<p><img src=\"/2022/06/12/MPI%E5%AD%A6%E4%B9%A0(%E4%B8%80)-%E7%AE%80%E5%8D%95%E7%9A%84%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6/mpi002.png\"></p>\n<h2 id=\"程序运行平台\"><a href=\"#程序运行平台\" class=\"headerlink\" title=\"程序运行平台\"></a>程序运行平台</h2><p>北京超级云计算中心A3分区</p>\n<h2 id=\"环境变量\"><a href=\"#环境变量\" class=\"headerlink\" title=\"环境变量\"></a>环境变量</h2><p>mpi&#x2F;intel&#x2F;2017.5</p>\n<h2 id=\"编译指令\"><a href=\"#编译指令\" class=\"headerlink\" title=\"编译指令\"></a>编译指令</h2><p>mpicc  3.1.cpp -o 3.1</p>\n<h2 id=\"运行指令\"><a href=\"#运行指令\" class=\"headerlink\" title=\"运行指令\"></a>运行指令</h2><p>mpirun -np 4 .&#x2F;3.1(本地提交,采取4个进程)</p>\n<h2 id=\"程序源代码\"><a href=\"#程序源代码\" class=\"headerlink\" title=\"程序源代码\"></a>程序源代码</h2><pre><code class=\"c++\">#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;mpi.h&gt;   //头文件\n\nconst int MAX_STRING = 100 ;\n\nint main(int argc,char **argv)\n&#123;\n    char greeting[MAX_STRING];\n    int comm_sz;\n    int my_rank;\n    MPI_Status status;\n\n    MPI_Init(&amp;argc,&amp;argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD,&amp;comm_sz);\n    MPI_Comm_rank(MPI_COMM_WORLD,&amp;my_rank);\n    \n    if(my_rank != 0)\n    &#123;\n        //其他进程向0号进程发消息\n        sprintf(greeting,&quot;Greetings from process %d of %d!&quot;,my_rank,comm_sz);\n        MPI_Send(greeting,strlen(greeting)+1,MPI_CHAR,0,0,MPI_COMM_WORLD); \n    &#125;\n    else\n    &#123;\n        printf(&quot;Greetings from process %d of %d!\\n&quot;,my_rank,comm_sz);\n        //0号进程接受来自其他进程的消息并输出\n        for(int q=1;q&lt;comm_sz;q++)\n        &#123;\n            MPI_Recv(greeting,MAX_STRING,MPI_CHAR,q,0,MPI_COMM_WORLD,&amp;status);\n            printf(&quot;%s\\n&quot;,greeting);\n        &#125;\n    &#125;\n    MPI_Finalize();\n    \n    return 0;\n&#125;\n</code></pre>\n<h2 id=\"程序运行结果\"><a href=\"#程序运行结果\" class=\"headerlink\" title=\"程序运行结果\"></a>程序运行结果</h2><pre><code class=\"c++\">Greetings from process 0 of 4!\nGreetings from process 1 of 4!\nGreetings from process 2 of 4!\nGreetings from process 3 of 4!\n</code></pre>\n",
            "tags": [
                "MPI"
            ]
        },
        {
            "id": "http://example.com/2022/04/02/mpi006/",
            "url": "http://example.com/2022/04/02/mpi006/",
            "title": "MPI学习(六)-两个矩阵相加MPI版本",
            "date_published": "2022-04-02T08:07:46.000Z",
            "content_html": "<h1 id=\"MPI学习-六-两个矩阵相加MPI版本\"><a href=\"#MPI学习-六-两个矩阵相加MPI版本\" class=\"headerlink\" title=\"MPI学习(六)-两个矩阵相加MPI版本\"></a>MPI学习(六)-两个矩阵相加MPI版本</h1><p>这里，我们演示了两个简单的程序，一个是矩阵相加串行版本，一个是矩阵相加MPI版本</p>\n<h2 id=\"串行版本\"><a href=\"#串行版本\" class=\"headerlink\" title=\"串行版本\"></a>串行版本</h2><h3 id=\"程序源代码\"><a href=\"#程序源代码\" class=\"headerlink\" title=\"程序源代码\"></a>程序源代码</h3><pre><code class=\"c++\">#include&lt;stdio.h&gt;\n#include&lt;string.h&gt;\nint main()\n&#123;\n    int a[4][4]=&#123;1,2,3,4,\n                5,6,7,8,\n                9,10,11,12,\n                13,14,15,16&#125;;\n    int b[4][4]=&#123;4,2,5,7,\n                1,3,8,9,\n                11,12,17,5,\n                15,14,20,3&#125;;\n    int c[4][4];\n    memset(c,0,sizeof(c)); \n    for(int i=0;i&lt;4;i++)\n    &#123;\n        for(int j=0;j&lt;4;j++)\n        &#123;\n            c[i][j]=a[i][j]+b[i][j];\n        &#125;\n    &#125;\n    for(int i=0;i&lt;4;i++)\n    &#123;\n        for(int j=0;j&lt;4;j++)\n        &#123;\n            printf(&quot;%d &quot;,c[i][j]);\n        &#125;\n        printf(&quot;\\n&quot;);\n    &#125;\n    return 0;\n&#125;\n</code></pre>\n<h3 id=\"程序输出\"><a href=\"#程序输出\" class=\"headerlink\" title=\"程序输出\"></a>程序输出</h3><pre><code class=\"c++\">5 4 8 11\n6 9 15 17\n20 22 28 17\n28 28 35 19\n</code></pre>\n<h2 id=\"MPI版本\"><a href=\"#MPI版本\" class=\"headerlink\" title=\"MPI版本\"></a>MPI版本</h2><h3 id=\"程序运行平台\"><a href=\"#程序运行平台\" class=\"headerlink\" title=\"程序运行平台\"></a>程序运行平台</h3><p>北京超级云计算中心A3分区</p>\n<h3 id=\"环境变量\"><a href=\"#环境变量\" class=\"headerlink\" title=\"环境变量\"></a>环境变量</h3><p>mpi&#x2F;intel&#x2F;2017.5</p>\n<h3 id=\"编译指令\"><a href=\"#编译指令\" class=\"headerlink\" title=\"编译指令\"></a>编译指令</h3><p>mpicxx mpi006.c -o mpi006</p>\n<h3 id=\"运行指令\"><a href=\"#运行指令\" class=\"headerlink\" title=\"运行指令\"></a>运行指令</h3><p>srun -p amd_256 -N 1 -n  5  .&#x2F;mpi006(使用SLURM任务调度系统)</p>\n<p>1个分区，核数为5</p>\n<h3 id=\"程序源代码-1\"><a href=\"#程序源代码-1\" class=\"headerlink\" title=\"程序源代码\"></a>程序源代码</h3><pre><code class=\"c++\">#include&lt;stdio.h&gt;\n#include&lt;string.h&gt;\n#include&lt;mpi.h&gt;\nint main(int argc ,char **argv)\n&#123;\n    int a[4][4]=&#123;1,2,3,4,\n                5,6,7,8,\n                9,10,11,12,\n                13,14,15,16&#125;;\n    int b[4][4]=&#123;4,2,5,7,\n                1,3,8,9,\n                11,12,17,5,\n                15,14,20,3&#125;;\n    int c[4][4];\n    int tmp[4];\n    memset(c,0,sizeof(c));\n    int myid, numprocs;\n    MPI_Status status;\n    MPI_Request request;\n    MPI_Init(&amp;argc,&amp;argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&amp;numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&amp;myid);\n    if(myid == 0)\t\t\t\t\t\t\t\t\t\t//0号进程接受来自其他进程的消息\n    &#123;\n        for(int i=1;i&lt;numprocs;i++)\n        &#123;\n            MPI_Irecv(&amp;c[i-1][0],4,MPI_INT,i,0,MPI_COMM_WORLD,&amp;request);\t\t//采用非阻塞接受\n            MPI_Wait(&amp;request,&amp;status);\n        &#125;\n        for(int i=0;i&lt;4;i++)\t\t\t\t\t\t\t//打印矩阵\n        &#123;\n            \n            for(int j=0;j&lt;4;j++)\n            &#123;\n                printf(&quot;%d &quot;,c[i][j]);\n            &#125;\n            printf(&quot;\\n&quot;);\n        &#125;\n    &#125;\n    if(myid != 0 )\t\t\t\t\t\t\t\t\t\t//当进程不是0号进程时，则向0号进程发送消息\n    &#123;\n        memset(tmp,0,sizeof(tmp));\t\t\t\t\t\t//初始化tmp数组\n        for(int i=0;i&lt;4;i++)\n        &#123;\n            tmp[i]=a[myid-1][i]+b[myid-1][i];\t\t\t//用tmp来临时存储相加结果，随后发送给0号进程\n            \n        &#125;\n        MPI_Isend(&amp;tmp,4,MPI_INT,0,0,MPI_COMM_WORLD,&amp;request);\t\t\t\t\t//采用非阻塞发送\n        MPI_Wait(&amp;request,&amp;status);\n    &#125;\n    MPI_Finalize();\t\n    return 0;\n&#125;\n</code></pre>\n<h3 id=\"程序输出-1\"><a href=\"#程序输出-1\" class=\"headerlink\" title=\"程序输出\"></a>程序输出</h3><pre><code class=\"c++\">5 4 8 11 \n6 9 15 17 \n20 22 28 17 \n28 28 35 19\n</code></pre>\n",
            "tags": [
                "MPI"
            ]
        },
        {
            "id": "http://example.com/2022/01/01/mpi005/",
            "url": "http://example.com/2022/01/01/mpi005/",
            "title": "MPI学习(五)-环形拓扑上利用MPI进行通信",
            "date_published": "2022-01-01T09:01:47.000Z",
            "content_html": "<h1 id=\"MPI学习-五-环形拓扑上利用MPI进行通信\"><a href=\"#MPI学习-五-环形拓扑上利用MPI进行通信\" class=\"headerlink\" title=\"MPI学习(五)-环形拓扑上利用MPI进行通信\"></a>MPI学习(五)-环形拓扑上利用MPI进行通信</h1><p>这里，我们演示了一个简单的MPI程序，它使用阻塞通信原语send和receive来进行广播操作：</p>\n<h2 id=\"程序运行平台\"><a href=\"#程序运行平台\" class=\"headerlink\" title=\"程序运行平台\"></a>程序运行平台</h2><p>北京超级云计算中心A3分区</p>\n<h2 id=\"环境变量\"><a href=\"#环境变量\" class=\"headerlink\" title=\"环境变量\"></a>环境变量</h2><p>mpi&#x2F;intel&#x2F;2017.5</p>\n<h2 id=\"编译指令\"><a href=\"#编译指令\" class=\"headerlink\" title=\"编译指令\"></a>编译指令</h2><p>mpic++ mpi005.cpp -o mpi005</p>\n<h2 id=\"运行指令\"><a href=\"#运行指令\" class=\"headerlink\" title=\"运行指令\"></a>运行指令</h2><p>srun -p amd_256 -N 1 -n  4   .&#x2F;mpi005(使用SLURM任务调度系统)</p>\n<p>1个分区，核数为4</p>\n<h2 id=\"程序源代码\"><a href=\"#程序源代码\" class=\"headerlink\" title=\"程序源代码\"></a>程序源代码</h2><pre><code class=\"c++\">#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\nint main(int argc,char *argv[])\n&#123;\n    int rank, value, size;\n    MPI_Status status;\n    MPI_Init(&amp;argc, &amp;argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n    \n    if(rank == 0) &#123;scanf(&quot;%d&quot;, &amp;value );\n    /*Master node sends out the value*/\n    MPI_Send(&amp;value, 1, MPI_INT, rank + 1, 0, MPI_COMM_WORLD);&#125;//\n    else\n    &#123;\n            /*Slave nodes block on receive the send on the value*/\n        //接受上一个进程发送的消息\n        MPI_Recv(&amp;value, 1, MPI_INT, rank - 1, 0,MPI_COMM_WORLD, &amp;status);\n            \n        if(rank &lt; size-1)\n        &#123;\n            //向下一个进程发送消息\n            MPI_Send(&amp;value, 1, MPI_INT, rank + 1, 0, MPI_COMM_WORLD);\n        &#125;\n        printf(&quot;process %d got %d\\n&quot;, rank, value);\n    &#125;\n    MPI_Finalize();\n    return 0;\n &#125; \n</code></pre>\n<h2 id=\"程序运行结果\"><a href=\"#程序运行结果\" class=\"headerlink\" title=\"程序运行结果\"></a>程序运行结果</h2><pre><code class=\"c++\">//5 5为读入的value值\nprocess 1 got 5\nprocess 2 got 5\nprocess 3 got 5\n</code></pre>\n",
            "tags": [
                "MPI"
            ]
        },
        {
            "id": "http://example.com/2021/11/27/mpi004/",
            "url": "http://example.com/2021/11/27/mpi004/",
            "title": "MPI学习(四)-广播、散播、收集、归约和全归约的MPI语法",
            "date_published": "2021-11-27T08:58:36.000Z",
            "content_html": "<h1 id=\"MPI-广播、散播、收集、归约和全归约的MPI语法\"><a href=\"#MPI-广播、散播、收集、归约和全归约的MPI语法\" class=\"headerlink\" title=\"MPI-广播、散播、收集、归约和全归约的MPI语法\"></a>MPI-广播、散播、收集、归约和全归约的MPI语法</h1><h2 id=\"广播：MPI-Bcast\"><a href=\"#广播：MPI-Bcast\" class=\"headerlink\" title=\"广播：MPI_Bcast\"></a>广播：MPI_Bcast</h2><pre><code class=\"c++\">int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)\n//int root 广播数据的根进程的标识号(整型)\n</code></pre>\n<p><img src=\"/2021/11/27/mpi004/mpi001.png\"></p>\n<h2 id=\"散播：MPI-Scatter\"><a href=\"#散播：MPI-Scatter\" class=\"headerlink\" title=\"散播：MPI_Scatter\"></a>散播：MPI_Scatter</h2><pre><code class=\"c++\">int MPI_Scatter(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int root, MPI_Comm comm)\n//void *sendbuf 发送消息缓冲区的起始地址(可选数据类型)\n//void *recvbuf 接收消息缓冲区的起始地址(可选数据类型)\n</code></pre>\n<p><img src=\"/2021/11/27/mpi004/mpi002.png\"></p>\n<h2 id=\"收集：MPI-Gather\"><a href=\"#收集：MPI-Gather\" class=\"headerlink\" title=\"收集：MPI_Gather\"></a>收集：MPI_Gather</h2><pre><code class=\"c++\">int MPI_Gather(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)\n</code></pre>\n<p><img src=\"/2021/11/27/mpi004/mpi003.png\"></p>\n<h2 id=\"归约：MPI-Reduce\"><a href=\"#归约：MPI-Reduce\" class=\"headerlink\" title=\"归约：MPI_Reduce\"></a>归约：MPI_Reduce</h2><pre><code class=\"c++\">int MPI_Reduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root,  MPI_Comm comm)\n//MPI_Op op  归约操作符(句柄)\n</code></pre>\n<p><img src=\"/2021/11/27/mpi004/mpi004.png\"></p>\n<p><img src=\"/2021/11/27/mpi004/mpi005.png\"></p>\n<h2 id=\"全归约：MPI-Allreduce\"><a href=\"#全归约：MPI-Allreduce\" class=\"headerlink\" title=\"全归约：MPI_Allreduce\"></a>全归约：MPI_Allreduce</h2><pre><code class=\"c++\">int MPI_Allreduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)\n</code></pre>\n<p><img src=\"/2021/11/27/mpi004/mpi006.png\"></p>\n",
            "tags": [
                "MPI"
            ]
        },
        {
            "id": "http://example.com/2021/11/26/mpi003/",
            "url": "http://example.com/2021/11/26/mpi003/",
            "title": "MPI学习(三)-通过OpenmMP使用MPI",
            "date_published": "2021-11-26T14:09:37.000Z",
            "content_html": "<h1 id=\"MPI-通过OpenMP使用MPI\"><a href=\"#MPI-通过OpenMP使用MPI\" class=\"headerlink\" title=\"MPI-通过OpenMP使用MPI\"></a>MPI-通过OpenMP使用MPI</h1><p>OpenMP是另一种为基于共享内存的并行编程提供的应用编程接口。当人们想使用多核处理器时，通常使用OpenMP。下面是一个“Hello World”程序，使用了MPI和OpenMP的API。</p>\n<h2 id=\"程序运行平台\"><a href=\"#程序运行平台\" class=\"headerlink\" title=\"程序运行平台\"></a>程序运行平台</h2><p>北京超级云计算中心A3分区</p>\n<h2 id=\"环境变量\"><a href=\"#环境变量\" class=\"headerlink\" title=\"环境变量\"></a>环境变量</h2><p>mpi&#x2F;intel&#x2F;2017.5</p>\n<h2 id=\"编译指令\"><a href=\"#编译指令\" class=\"headerlink\" title=\"编译指令\"></a>编译指令</h2><p>mpic++    -fopenmp  mpi003.cpp -o mpi003</p>\n<h2 id=\"运行指令\"><a href=\"#运行指令\" class=\"headerlink\" title=\"运行指令\"></a>运行指令</h2><p>srun -p amd_256 -N 2 -n  2   .&#x2F;mpi003(使用SLURM任务调度系统)</p>\n<p>2个分区，核数为2</p>\n<h2 id=\"程序源代码\"><a href=\"#程序源代码\" class=\"headerlink\" title=\"程序源代码\"></a>程序源代码</h2><pre><code class=\"c++\">#include &lt;math.h&gt;\n#include &lt;omp.h&gt;  //OpenMP所需要的头文件\n#include &lt;mpi.h&gt;\nint  main(int argc,char **argv)\n&#123;\n        int myid, numprocs;\n        int namelen;\n        int thread_id , nthreads;\n        char processor_name[MPI_MAX_PROCESSOR_NAME];\n        MPI_Init(&amp;argc, &amp;argv);\n        MPI_Comm_rank(MPI_COMM_WORLD, &amp;myid);\n        MPI_Comm_size(MPI_COMM_WORLD, &amp;numprocs);\n        MPI_Get_processor_name(processor_name, &amp;namelen);\n        //构造并行区\n        #pragma omp parallel private(thread_id, nthreads) num_threads(8) //设置线程数为8\n        &#123;\n            thread_id = omp_get_thread_num();  //获得当前线程的id\n            nthreads = omp_get_num_threads();  //获得总的线程数\n            printf(&quot;Thread number %d (on %d) for the MPI process number %d (on %d) [%s]\\n&quot;,\n            thread_id, nthreads, myid, numprocs, processor_name);\t\n        &#125; \t\t\n        MPI_Finalize();\n        return 0;\n&#125;\n</code></pre>\n<h2 id=\"程序运行结果\"><a href=\"#程序运行结果\" class=\"headerlink\" title=\"程序运行结果\"></a>程序运行结果</h2><pre><code class=\"c++\">Thread number 0 (on 8) for the MPI process number 1 (on 2) [eb1316.para.bscc]\nThread number 4 (on 8) for the MPI process number 1 (on 2) [eb1316.para.bscc]\nThread number 3 (on 8) for the MPI process number 1 (on 2) [eb1316.para.bscc]\nThread number 5 (on 8) for the MPI process number 1 (on 2) [eb1316.para.bscc]\nThread number 2 (on 8) for the MPI process number 1 (on 2) [eb1316.para.bscc]\nThread number 6 (on 8) for the MPI process number 1 (on 2) [eb1316.para.bscc]\nThread number 0 (on 8) for the MPI process number 0 (on 2) [eb1314.para.bscc]\nThread number 1 (on 8) for the MPI process number 0 (on 2) [eb1314.para.bscc]\nThread number 2 (on 8) for the MPI process number 0 (on 2) [eb1314.para.bscc]\nThread number 5 (on 8) for the MPI process number 0 (on 2) [eb1314.para.bscc]\nThread number 1 (on 8) for the MPI process number 1 (on 2) [eb1316.para.bscc]\nThread number 7 (on 8) for the MPI process number 1 (on 2) [eb1316.para.bscc]\nThread number 3 (on 8) for the MPI process number 0 (on 2) [eb1314.para.bscc]\nThread number 4 (on 8) for the MPI process number 0 (on 2) [eb1314.para.bscc]\nThread number 6 (on 8) for the MPI process number 0 (on 2) [eb1314.para.bscc]\nThread number 7 (on 8) for the MPI process number 0 (on 2) [eb1314.para.bscc]\n</code></pre>\n",
            "tags": [
                "MPI"
            ]
        },
        {
            "id": "http://example.com/2021/11/26/mpi002/",
            "url": "http://example.com/2021/11/26/mpi002/",
            "title": "MPI学习(二)-非阻塞通信程序(无缓冲)",
            "date_published": "2021-11-26T09:44:34.000Z",
            "content_html": "<h1 id=\"MPI-非阻塞通信程序-无缓冲\"><a href=\"#MPI-非阻塞通信程序-无缓冲\" class=\"headerlink\" title=\"MPI-非阻塞通信程序(无缓冲)\"></a>MPI-非阻塞通信程序(无缓冲)</h1><p>非阻塞通信程序(无缓冲)是由MPI中的Isend和Ireceive来表示的，即异步通信。在这种情况下，发送进程发布一条“发送授权请求”(挂起的消息)的消息，并继续其程序的执行。当接收进程发布一个“同意发送”许可指令时，数据传输就启动了。所有的这些机制都是通过操作系统的信号进行内部管理的。当数据传输完成时，检查状态并指示进程是否可以安全地进行读&#x2F;写数据。                                                                      需要注意的是原语MPI_Wait(&amp;request,&amp;status)等到数据传输完成(或中断后)，使用一个成为status的状态变量来指示数据传输是否已经成功。</p>\n<h2 id=\"相关的MPI原语\"><a href=\"#相关的MPI原语\" class=\"headerlink\" title=\"相关的MPI原语\"></a>相关的MPI原语</h2><pre><code class=\"c++\">int MPI_Isend(void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_comm comm, MPI_Request *req)\nint MPI_Irecv(void *buf, int count, MPI_Datatype datatype, int src, int tag, MPI_Comm comm, MPI_Request *req)\n</code></pre>\n<p>MPI_Request结构中经常使用的：当*req操作完成时返回 *flag &#x3D; 1,否则返回 0。</p>\n<p>原语MPI_Wait一直等到*req所执行的操作完成。</p>\n<pre><code class=\"c++\">int MPI_Wait(MPI_Request *req, MPI_Status *status)\n</code></pre>\n<h2 id=\"程序运行平台\"><a href=\"#程序运行平台\" class=\"headerlink\" title=\"程序运行平台\"></a>程序运行平台</h2><p>北京超级云计算中心A3分区</p>\n<h2 id=\"环境变量\"><a href=\"#环境变量\" class=\"headerlink\" title=\"环境变量\"></a>环境变量</h2><p>mpi&#x2F;intel&#x2F;2017.5</p>\n<h2 id=\"编译指令\"><a href=\"#编译指令\" class=\"headerlink\" title=\"编译指令\"></a>编译指令</h2><p>mpicc  mpi002.cpp -o mpi002</p>\n<h2 id=\"运行指令\"><a href=\"#运行指令\" class=\"headerlink\" title=\"运行指令\"></a>运行指令</h2><p>mpirun -np 10   .&#x2F;mpi002(本地提交,采取10个进程)</p>\n<h2 id=\"程序源代码\"><a href=\"#程序源代码\" class=\"headerlink\" title=\"程序源代码\"></a>程序源代码</h2><pre><code class=\"c++\">#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;math.h&gt;\nint main(int argc,char **argv)\n&#123;\n    int myid, numprocs;\n    int tag,source,destination,count;\n    int buffer;\n    MPI_Status status;\n    MPI_Request request;\n    MPI_Init(&amp;argc,&amp;argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&amp;numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&amp;myid);\n    tag =2021; /* any integer to tag messages */\n    source = 0;\n    count = 1;\n    if(myid != source )\n    &#123;\n        buffer =2077;\n        //其他进程向0号进程发送消息\n        MPI_Isend(&amp;buffer,count,MPI_INT,source,tag,MPI_COMM_WORLD,&amp;request);\n        MPI_Wait(&amp;request,&amp;status);\n        printf(&quot;processor %d send %d to processor %d\\n&quot;,myid,buffer,source);\n    &#125;\n\n    if(myid == source )\n    &#123;\n        //0号进程接收来自其他进程的消息\n        for(int i=1;i&lt;numprocs;i++)\n        &#123;\n            MPI_Irecv(&amp;buffer,count,MPI_INT,i,tag,MPI_COMM_WORLD,&amp;request);\n            MPI_Wait(&amp;request,&amp;status);\n            printf(&quot;processor %d received %d from processor %d \\n&quot;,myid,buffer,i);\n        &#125;\n        \n    &#125;\n\n    MPI_Finalize();\n    return 0;\n&#125;\n</code></pre>\n<h2 id=\"程序输出\"><a href=\"#程序输出\" class=\"headerlink\" title=\"程序输出\"></a>程序输出</h2><pre><code class=\"c++\">processor 2 send 2077 to processor 0\nprocessor 3 send 2077 to processor 0\nprocessor 5 send 2077 to processor 0\nprocessor 6 send 2077 to processor 0\nprocessor 7 send 2077 to processor 0\nprocessor 8 send 2077 to processor 0\nprocessor 9 send 2077 to processor 0\nprocessor 4 send 2077 to processor 0\nprocessor 1 send 2077 to processor 0\nprocessor 0 received 2077 from processor 1 \nprocessor 0 received 2077 from processor 2 \nprocessor 0 received 2077 from processor 3 \nprocessor 0 received 2077 from processor 4 \nprocessor 0 received 2077 from processor 5 \nprocessor 0 received 2077 from processor 6 \nprocessor 0 received 2077 from processor 7 \nprocessor 0 received 2077 from processor 8 \nprocessor 0 received 2077 from processor 9 \n</code></pre>\n",
            "tags": [
                "MPI"
            ]
        }
    ]
}